{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyD1Qqhbyu4Q"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from itertools import product\n",
    "from wandb.keras import WandbCallback\n",
    "from numba import jit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Input, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Huber, MeanSquaredError\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "\n",
    "from spektral.layers import XENetDenseConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import Graph\n",
    "from spektral.data.dataset import Dataset\n",
    "from spektral.data.loaders import BatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": 'v2 - 1st Run',\n",
    "    'method':'bayes'\n",
    "}\n",
    "\n",
    "metric = {\n",
    "    'name' : 'loss',\n",
    "    'goal' : 'minimize'\n",
    "}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    'N_xenet': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 1,\n",
    "        'max': 3\n",
    "    },\n",
    "    'N_dense': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 0,\n",
    "        'max': 5\n",
    "    },\n",
    "    'division_factor_dense': {\n",
    "        'value': 1\n",
    "    },\n",
    "    'p_drop': {\n",
    "        'value': 0\n",
    "    },\n",
    "    'stack_channels': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 200\n",
    "    },\n",
    "    'node_channels': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 200\n",
    "    },\n",
    "    'edge_channels': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 200\n",
    "    },\n",
    "    'train_episodes': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 50,\n",
    "        'max': 150\n",
    "    },    \n",
    "    'MIN_REPLAY_SIZE': {\n",
    "        'value': 200\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 10,\n",
    "        'max': 200\n",
    "    },\n",
    "    'step_buffer': {\n",
    "        'value': 8\n",
    "    },\n",
    "    'step_buffer_set': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 8\n",
    "    },\n",
    "    'set_target': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 3,\n",
    "        'max': 50\n",
    "    },\n",
    "    'decay' : {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.0001,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'discount_factor' : {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.1,\n",
    "        'max': 0.9\n",
    "    },\n",
    "    'lr' : {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.0001,\n",
    "        'max': 0.01\n",
    "    }\n",
    "}\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"BayesSearch - D=2 & L=5\", entity=\"locp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R40YBY74yJX9"
   },
   "source": [
    "## Predefined Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKqMVduAyJYA"
   },
   "source": [
    "### Adjacency Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zs9snfcSyJYB"
   },
   "outputs": [],
   "source": [
    "def Adj(D, L, sparse=False):\n",
    "    N = L**D\n",
    "\n",
    "    # create all nodes' coordinates\n",
    "    nodes = [x for x in np.ndindex(tuple(np.repeat(L,D)))]\n",
    "\n",
    "    # Pass from coordinate to node's index\n",
    "    # (h,...k,j,i) <=> index = h*L^(D-1) + ... + k*L^2 + j*L + i\n",
    "    mul = [L**i for i in reversed(range(D))]\n",
    "\n",
    "    # Creation of adjacency matrix \n",
    "    A_dense = []\n",
    "    # creation of a row for each node's coordinate \n",
    "    for node in nodes:       \n",
    "        temp_buffer = []\n",
    "        A_dense_row = [0]*N\n",
    "        # find the two nearest neighbours of the node along each dimension\n",
    "        for d in range(D):\n",
    "            temp=list(node)\n",
    "            temp[d]=((temp[d]+1)%L)\n",
    "            temp=np.inner(temp, mul)\n",
    "            temp_buffer.append(temp)    \n",
    "\n",
    "            temp=list(node)\n",
    "            temp[d]=((temp[d]-1)%L)\n",
    "            temp=np.inner(temp, mul)\n",
    "            temp_buffer.append(temp)\n",
    "      \n",
    "        temp_buffer=list(np.unique(np.array(temp_buffer), axis=0))   \n",
    "        for i in temp_buffer: A_dense_row[i]=1\n",
    "        A_dense.append(A_dense_row)\n",
    "    \n",
    "    # sparse=False => sparse adjacency matrix\n",
    "    # sparse=True => dense adjacency matrix\n",
    "    if sparse:\n",
    "        return sp_matrix_to_sp_tensor(np.array(A_dense))\n",
    "    else:\n",
    "        return np.array(A_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj5_2iJOyJYE"
   },
   "source": [
    "### Interaction Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFHBU3C6yJYF"
   },
   "outputs": [],
   "source": [
    "def J_inter(denseAdj):\n",
    "    N = denseAdj.shape[0]\n",
    "    sparseAdj = sp_matrix_to_sp_tensor(denseAdj)\n",
    "\n",
    "    # sparse adjacency matrix as a numpy array\n",
    "    edge=sparseAdj.indices.numpy()\n",
    "\n",
    "    # ordered numpy sparse adjacency matrix\n",
    "    un_edge=np.array([np.sort(i) for i in edge]) \n",
    "\n",
    "    # creation of the interaction array: (i,j) and (j,i) have the same Jij\n",
    "    inter=[]\n",
    "    for i in range(len(un_edge)):\n",
    "        equal=True\n",
    "        for j in range(i):\n",
    "            if np.array_equal(un_edge[i],un_edge[j]):\n",
    "                inter.append(inter[j])\n",
    "                equal=False\n",
    "                break\n",
    "        if equal: \n",
    "            inter.append(np.random.normal(0, 1))\n",
    "    \n",
    "    # creation of dense interaction matrix\n",
    "    inter_matrix = np.zeros((N,N))\n",
    "    counter = 0\n",
    "    for i, j in edge:\n",
    "        inter_matrix[i,j] = inter[counter]\n",
    "        counter += 1\n",
    "    return [np.array(inter).reshape(sparseAdj.indices.shape[0],1), inter_matrix.reshape((N,N,1))]\n",
    "    \n",
    "    # index of the returned list:\n",
    "    # 0 => interaction array\n",
    "    # 1 => interaction matrix (zero padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcYp--P9yJYH"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMEfE3YyyJYJ"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, N_graph, X, Y, A, E, **kwargs):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.N_graph = N_graph\n",
    "        self.A = A\n",
    "        self.E = E\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        mydataset = []\n",
    "        for i in range(self.N_graph):\n",
    "            # list of Graph objects that will be used as input in the BatchLoader\n",
    "            mydataset.append(\n",
    "                    Graph(x=self.X[i], a=self.A[i], e=self.E[i], y=self.Y[i])      \n",
    "                    )\n",
    "        return mydataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKMlL7aXyJYK"
   },
   "source": [
    "### Replay Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwfwO3odyJYM"
   },
   "outputs": [],
   "source": [
    "# save n-step transitions (s_t; a_t; r_t,t+n; s_t+n) from the trajectory buffer\n",
    "\n",
    "def get_replay_memory(trajectory_buffer, replay_memory):\n",
    "    n = len(trajectory_buffer)\n",
    "\n",
    "    states = np.array([transition[0] for transition in trajectory_buffer])\n",
    "    actions = np.array([transition[1] for transition in trajectory_buffer])\n",
    "    rewards = np.array([transition[2] for transition in trajectory_buffer])\n",
    "    done = np.array([transition[4] for transition in trajectory_buffer])\n",
    "    inter_matrix = trajectory_buffer[0][-1]\n",
    "    dense_AdjMat = trajectory_buffer[0][-2]\n",
    "\n",
    "    cum_reward = np.cumsum(rewards)\n",
    "\n",
    "    # creating the replay memory buffer from the trajectory one\n",
    "    # => (starting state, action performed, cumulative reward after n step from the starting one, state after n step from the starting one, episode ended, dense adjacency matrix of the episode, interaction matrix of the episode)\n",
    "    replay_memory.append([states[0], actions[0], cum_reward[n-1], states[n-1], done[n-1], dense_AdjMat, inter_matrix])\n",
    "\n",
    "    return replay_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEY_CY7VyJYN"
   },
   "source": [
    "### Real Energy Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhjfrmwhyJYN"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def computeEnergy(state, edge, interaction):  \n",
    "    energy = 0\n",
    "    for i in range(len(edge)):\n",
    "        energy -= interaction[i][0]*state[edge[i][0]][0]*state[edge[i][1]][0]\n",
    "    return energy/2\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def EnergyMinima(states, inter, sparseAdj):\n",
    "    # find the minimum energy for a fixed J_ij configuration\n",
    "    energy_min = np.inf\n",
    "    for state in states:\n",
    "        state = np.array(state).reshape(N,1)\n",
    "        state_energy = computeEnergy(state, sparseAdj, inter)\n",
    "        if state_energy<energy_min: energy_min=state_energy\n",
    "    return energy_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7GwjAGayJYO"
   },
   "source": [
    "## Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBckJ6iuyJYP"
   },
   "outputs": [],
   "source": [
    "class SG_env(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self, L, D):\n",
    "    # Initialize environment attributes\n",
    "    # - action_spec: action declaration (integer from 0 to N-1)\n",
    "    # - observation_spec: state of the system declaration\n",
    "    # - episode_ended: flag for the end of the episode (all spin down)\n",
    "    # - state: state of the system (1 and -1 array)\n",
    "    self.N = L**D\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=self.N-1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(self.N,D+1), dtype=np.int32, minimum=-1, maximum=self.N-1, name='observation')\n",
    "    self.sp_AdjMat = Adj(D, L, sparse=True)\n",
    "    self.dense_AdjMat = Adj(D, L, sparse=False)\n",
    "    list_J = J_inter(self.dense_AdjMat)\n",
    "    self.interaction = list_J[0]   \n",
    "    self.inter_matrix = list_J[1]\n",
    "    nodes = [x for x in np.ndindex(tuple(np.repeat(L,2)))]\n",
    "    self._state = np.append(np.append(np.ones(shape=(self.N,1)), [[node[0]] for node in nodes], axis=1), [[node[1]] for node in nodes], axis=1).astype(\"int32\")\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def get_state(self):\n",
    "    return self._state\n",
    "\n",
    "  def show_N(self):\n",
    "    return self.N  \n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def show_dense_AdjMat(self):\n",
    "    return self.dense_AdjMat\n",
    "\n",
    "  def show_sp_AdjMat(self):\n",
    "    return self.sp_AdjMat\n",
    "\n",
    "  def show_interaction(self):\n",
    "    return self.interaction\n",
    "\n",
    "  def show_inter_matrix(self):\n",
    "    return self.inter_matrix\n",
    "\n",
    "  # True => All spins = -1, False => otherwise\n",
    "  def __all_spins_down(self):\n",
    "    return np.all(self._state[:,0]==-1)    \n",
    "\n",
    "  # Compute the reward of the chosen action \n",
    "  # reward = energy difference between consecutive states\n",
    "  # nns => nearest neighbours indexes\n",
    "  # nn_Js => nearest neighbours interactions' indexes\n",
    "  def computeReward(self, action):\n",
    "    nns = self.sp_AdjMat.indices[self.sp_AdjMat.indices[:,0]==action][:,1].numpy()\n",
    "    nn_Js = np.where(self.sp_AdjMat.indices[:,0]==action)[0]\n",
    "    nn_sum = 0\n",
    "    for i in range(len(nns)): nn_sum += self.interaction[nn_Js[i]]*self._state[nns[i],0]\n",
    "    reward = 2*nn_sum*self._state[action,0]\n",
    "    return reward[0]\n",
    "\n",
    "  # Compute the energy of the current state \n",
    "  def computeEnergy(self):\n",
    "    edge = self.sp_AdjMat.indices.numpy()\n",
    "    Nedge = len(edge)\n",
    "    energy = 0\n",
    "    for i in range(Nedge):\n",
    "        energy -= self.interaction[i][0]*self._state[edge[i][0]][0]*self._state[edge[i][1]][0]\n",
    "    return energy/2\n",
    "\n",
    "  # reset function: called when all spins are -1 => new episode\n",
    "  #                                              => all spins up (=1) and new interaction matrix (if needed)\n",
    "  def _reset(self):\n",
    "    self._state[:,0] = 1\n",
    "    #self.interaction = J_inter(self.dense_AdjMat)[0]    \n",
    "    #self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "  # step function: describe the process of applying the action selected by the agent\n",
    "  # ts.restart, ts.transition and ts.termination return a timestep \n",
    "  # containing step_type, reward, discount and observation\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "\n",
    "    if self.__all_spins_down():\n",
    "      self._episode_ended = True\n",
    "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==1):\n",
    "      self._state[action,0]=-1\n",
    "      rew = self.computeReward(action)\n",
    "      \n",
    "      if self.__all_spins_down():\n",
    "          self._episode_ended = True\n",
    "          return ts.termination(np.array(self._state, dtype=np.int32), reward=rew)\n",
    "      else:\n",
    "          return ts.transition(np.array(self._state, dtype=np.int32), reward=rew)\n",
    "    \n",
    "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==-1):\n",
    "      raise ValueError('Each spin can be flipped only once!')\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 up to N-1 - Spin Flip!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGsbfJ__yJYQ"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rtWCa2syJYR"
   },
   "outputs": [],
   "source": [
    "# Agent (=> GNN+FNN) \n",
    "# N => number nodes\n",
    "# D => number dimensions\n",
    "# stack_channels => integer or list of integers, number of channels for the hidden layers\n",
    "# node_channels => integer, number of output channels for the nodes\n",
    "# edge_channels => integer, number of output channels for the edges\n",
    "# division_factor_dense => integer, gradually reduce the number of neurons for each dense layer\n",
    "# p_drop => float between 0 and 1, fraction of the input units to drop\n",
    "# N_xenet => Number of XENetDenseConv layers\n",
    "# N_dense => Number of Dense hidden layers\n",
    "\n",
    "def agent(N,                                 \n",
    "          D,                                 \n",
    "          stack_channels=5,        \n",
    "          node_channels=3,\n",
    "          edge_channels=3,\n",
    "          division_factor_dense=4,\n",
    "          p_drop=0,\n",
    "          N_xenet=2,\n",
    "          N_dense=2,\n",
    "          activation=\"relu\",\n",
    "          regularizer=0,\n",
    "          lr=0.001):\n",
    "  inX = Input(shape=(N,D+1), name='Input Nodes')\n",
    "  inA = Input(shape=(N,N), name='Input Adj matrix')\n",
    "  inE = Input(shape=(N,N,1), name='Input Edges')\n",
    "  \n",
    "  X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
    "                               attention=True, node_activation=activation, edge_activation=activation, \n",
    "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_0\")([inX, inA, inE])\n",
    "  for i in range(N_xenet-1):\n",
    "    X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
    "                               attention=True, node_activation=activation, edge_activation=activation, \n",
    "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_\"+str(i+1))([X, inA, E])\n",
    "  \n",
    "  # flat the updated X, E in order to feed the fully connected neural network (FNN)\n",
    "  flat_x, flat_e = Flatten(name=\"Nodes_encoding\")(X), Flatten(name=\"Edges_encoding\")(E)\n",
    "  out = Concatenate(axis=1, name=\"Concatenation\")([flat_x])    #,flat_e\n",
    "  \n",
    "  for i in range(N_dense):\n",
    "    out = Dense(out.shape.as_list()[1]//division_factor_dense, activation=activation, kernel_regularizer=l2(regularizer))(out)\n",
    "    out = Dropout(p_drop)(out)\n",
    "  out = Dense(N, activation=\"PReLU\", kernel_regularizer=l2(regularizer), name='Q-values')(out)\n",
    "  \n",
    "  model = Model([inX,inA,inE], out)\n",
    "  model.compile(optimizer=Adam(lr), loss=MeanSquaredError())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O_14QuiyJYS"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whlinbUayJYT"
   },
   "outputs": [],
   "source": [
    "def train(env, replay_memory, model, target_model, done, MIN_REPLAY_SIZE, batch_size, discount_factor):\n",
    "    \n",
    "    # skip the training if the number of samples in the replay \n",
    "    # memory is less than MIN_REPLAY_SIZE\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        print(\"\\t=> EXIT\\n\")\n",
    "        return\n",
    "\n",
    "    # randomly select a number of samples from the replay memory equal to batch_size\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "\n",
    "    # sets of all interaction matrix and dense adjacency matrix in the batch\n",
    "    E = np.array([transition[-1] for transition in mini_batch])\n",
    "    A = np.array([transition[-2] for transition in mini_batch])\n",
    "\n",
    "    # current_states => set of all starting observations in the batch\n",
    "    # current_qs_list => predicted Q-values of the current_states by the model\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = np.array(model.predict([current_states,A,E]))\n",
    " \n",
    "    # new_current_states => set of observations after performing n actions in the batch\n",
    "    # future_qs_list => predicted Q-values of the new_current_states by the target model\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = np.array(target_model.predict([new_current_states,A,E]))\n",
    "    \n",
    "    # X => observations\n",
    "    # Y => 'label' of each observation: updated current_qs_list \n",
    "    X = []\n",
    "    Y = []\n",
    "    for index, (observation, action, reward, new_observation, done, dense_AdjMat, inter_matrix) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            new_q = reward + discount_factor*np.max(future_qs_list[index])\n",
    "        else:\n",
    "            new_q = reward\n",
    "\n",
    "        # update the Q-value corrisponding to the performed action\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = new_q\n",
    "        \n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "\n",
    "    # build the training dataset \n",
    "    train_data = MyDataset(N_graph=batch_size, X=X, Y=Y, A=A, E=E)\n",
    "    # use the BatchLoader to fit the model and WandbCallback to load the loss to Weight&Biases\n",
    "    loader = BatchLoader(train_data, node_level=False, epochs=50, batch_size=batch_size, shuffle=False) \n",
    "    model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, verbose=2, callbacks = [WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOReIikvyJYU"
   },
   "outputs": [],
   "source": [
    "def main(env): \n",
    "    # set random seed for reproducibility\n",
    "    RANDOM_SEED = 5\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    \n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config \n",
    "        \n",
    "        L = 5\n",
    "        D = 2\n",
    "        env = SG_env(L=L, D=D)    \n",
    "    \n",
    "        # Epsilon-greedy algorithm in initialized at 1 \n",
    "        # => every step is random at the start\n",
    "        epsilon = 1          # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "        max_epsilon = 1      # You can't explore more than 100% of the time\n",
    "        min_epsilon = 0.01   # At a minimum, we'll always explore 1% of the time\n",
    "        decay = config.decay\n",
    "\n",
    "        # 1. Initialize the Target and Main models \n",
    "        # Main Model (updated every 3 steps)\n",
    "        model = agent(N=env.N, D=D, stack_channels=config.stack_channels,\n",
    "                      node_channels=config.node_channels, edge_channels=config.edge_channels,\n",
    "                      N_xenet=config.N_xenet, N_dense=config.N_dense, \n",
    "                      division_factor_dense=config.division_factor_dense, p_drop=config.p_drop, lr=config.lr)\n",
    "\n",
    "        # Target Model (updated at the end of every episode)\n",
    "        target_model = agent(N=env.N, D=D, stack_channels=config.stack_channels,\n",
    "                             node_channels=config.node_channels, edge_channels=config.edge_channels,\n",
    "                             N_xenet=config.N_xenet, N_dense=config.N_dense, \n",
    "                             division_factor_dense=config.division_factor_dense, p_drop=config.p_drop, lr=config.lr)\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "        energy_buffer = []\n",
    "        replay_memory = []\n",
    "        \n",
    "        # custom plot parameters\n",
    "        ep_ene_min_data = []\n",
    "        step_ene_min_data = []       \n",
    "\n",
    "        train_episodes = config.train_episodes\n",
    "        for episode in range(train_episodes):\n",
    "            # reset the variables at the beginning of an episode\n",
    "            trajectory_buffer = []   \n",
    "            ep_min_energy = np.inf\n",
    "            total_training_rewards = 0\n",
    "            steps_to_update_target_model = 0\n",
    "            env.reset()\n",
    "            #previous_obs = env.get_state()\n",
    "            previous_obs = np.ones(shape=(env.N,1)).astype(\"int32\")\n",
    "            done = False  \n",
    "            check = np.arange(0,env.N)\n",
    "\n",
    "            while not done: \n",
    "                observation = env.get_state()          \n",
    "                print(\"\\n\\n\\t\\t\\t\\t++++++++++++  episode:\", episode,\" - step:\", steps_to_update_target_model, \" ++++++++++++\")\n",
    "\n",
    "                # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "                random_number = np.random.rand()\n",
    "                if random_number <= epsilon:\n",
    "                    # Explore\n",
    "                    action = random.choice(check)\n",
    "\n",
    "                else:\n",
    "                    # Exploit best known action\n",
    "                    predicted = model([observation.reshape(1,env.N,3), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n",
    "                    while True:\n",
    "                        # check to prevent flipping the same spin twice - only once!\n",
    "                        action = np.argmax(predicted)\n",
    "                        if env.get_state()[action,0] == 1:\n",
    "                                break;\n",
    "                        predicted[action] = np.NINF\n",
    "\n",
    "                # remove the choosen action from check array\n",
    "                check = np.setdiff1d(check, action)\n",
    "\n",
    "                # perform the action on the environment and get the updated parameters\n",
    "                step_type, reward, discount, new_observation = env._step(action)\n",
    "                done = env._episode_ended\n",
    "                e = env.computeEnergy()\n",
    "\n",
    "                # save the parameter in the buffer\n",
    "                trajectory_buffer.append([previous_obs, action, reward, new_observation, done, e, env.dense_AdjMat, env.inter_matrix])\n",
    "                energy_buffer.append([episode, new_observation, env.interaction, e])  \n",
    "                if ep_min_energy>e: ep_min_energy = e\n",
    "\n",
    "                # load interesting parameters to weight&Biases\n",
    "                step_ene_min_data.append([e, episode*env.N+steps_to_update_target_model])\n",
    "                wandb.log({\n",
    "                    #\"Episode\": energy_buffer[episode*env.N+steps_to_update_target_model][0],\n",
    "                    #\"Step\": episode*env.N+steps_to_update_target_model,\n",
    "                    \"New observation\": wandb.Image(energy_buffer[episode*env.N+steps_to_update_target_model][1].reshape(L,L)),\n",
    "                    \"J interactions\": energy_buffer[episode*env.N+steps_to_update_target_model][2],\n",
    "                    #\"Energy\": energy_buffer[episode*env.N+steps_to_update_target_model][3]\n",
    "                })\n",
    "\n",
    "                \n",
    "                # fill the replay memory buffer\n",
    "                # => v1\n",
    "                if steps_to_update_target_model >= config.step_buffer:   \n",
    "                    replay_memory = get_replay_memory(trajectory_buffer, replay_memory)\n",
    "                    trajectory_buffer = trajectory_buffer[1:]\n",
    "                # => v2\n",
    "                #if steps_to_update_target_model >= config.step_buffer and steps_to_update_target_model <= config.step_buffer+config.step_buffer_set:\n",
    "                #    replay_memory = get_replay_memory(trajectory_buffer, replay_memory)\n",
    "                #    trajectory_buffer = trajectory_buffer[1:]\n",
    "                #elif steps_to_update_target_model > config.step_buffer+config.step_buffer_set:       \n",
    "                #    trajectory_buffer = trajectory_buffer[1:]\n",
    "\n",
    "                \n",
    "                # 3. Update the Main Network using the Bellman Equation  \n",
    "                #if (steps_to_update_target_model%L==0 and steps_to_update_target_model!=0) or done:\n",
    "                print(\"\\n\\t\\t\\t\\t\\t      +++++ Training +++++\")\n",
    "                train(env, replay_memory, model, target_model, done, config.MIN_REPLAY_SIZE, config.batch_size, config.discount_factor)\n",
    "\n",
    "                previous_obs = new_observation\n",
    "\n",
    "                # Copying main network weights to the target network\n",
    "                # weights at the end of the episode\n",
    "                if done:\n",
    "                    #if episode >= config.set_target:\n",
    "                    if episode%config.set_target==0:\n",
    "                        target_model.set_weights(model.get_weights())\n",
    "\n",
    "                steps_to_update_target_model += 1\n",
    "\n",
    "            # update epsilon using the following rule\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay*episode)\n",
    "            ep_ene_min_data.append([ep_min_energy, episode])\n",
    "        \n",
    "        # load interesting parameters to weight&Biases\n",
    "        wandb.log({\n",
    "            \"ep_ene_min_data\" : wandb.Table(data=ep_ene_min_data, columns=[\"energy minima\", \"episode\"]),\n",
    "            \"step_ene_min_data\" : wandb.Table(data=step_ene_min_data, columns=[\"energy\", \"step\"])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, main, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "XFeatures.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "61c4312e-360b-4641-8d4d-1b368b7458c8",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
