{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nimcuE3UyJXw",
        "outputId": "5b2cd623-984e-4c6b-a0dd-1c8cda0a6b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.21.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.17.3)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.26.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (14.0.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (4.2.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.46.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.9.1) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow==2.9.1) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spektral\n",
            "  Downloading spektral-1.1.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.21.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.26.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (14.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (21.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->spektral) (1.46.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->spektral) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.2.0->spektral) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->spektral) (3.1.0)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.13.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 26.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (4.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.17.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 122 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.21.6)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.1.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<=0.23.0,>=0.17.0->tf-agents) (0.16.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.1.7)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf-agents) (0.4.0)\n",
            "Installing collected packages: pygame, tf-agents\n",
            "Successfully installed pygame-2.1.0 tf-agents-0.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.18-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 23.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bd18e0a6c1c327aabb7a23d668f191186844894ceeb515bc395863a405ba6ec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.18\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install 'tensorflow==2.9.1'\n",
        "! pip install spektral\n",
        "! pip install tf-agents\n",
        "! pip install wandb\n",
        "! pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyD1Qqhbyu4Q"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from itertools import product\n",
        "from wandb.keras import WandbCallback\n",
        "from numba import jit\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Input, Flatten, Dense, Concatenate\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, Huber, MeanSquaredError\n",
        "from tensorflow.keras.metrics import categorical_accuracy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "\n",
        "from spektral.layers import XENetDenseConv\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "from spektral.data import Graph\n",
        "from spektral.data.dataset import Dataset\n",
        "from spektral.data.loaders import BatchLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "7E4pwrWEyJX7",
        "outputId": "c0e185e1-2690-47f8-a7f4-7c80f502b58b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.18"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220614_151828-f1r44mt2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/locp/Test%20-%20D%3D2%20%26%20L%3D6%2C%20XFeature/runs/f1r44mt2\" target=\"_blank\">sparkling-wave-8</a></strong> to <a href=\"https://wandb.ai/locp/Test%20-%20D%3D2%20%26%20L%3D6%2C%20XFeature\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/locp/Test%20-%20D%3D2%20%26%20L%3D6%2C%20XFeature/runs/f1r44mt2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f49c99a9cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# initialize wandb\n",
        "\n",
        "wandb.init(project=\"Test - D=2 & L=6, XFeature\", entity=\"locp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R40YBY74yJX9"
      },
      "source": [
        "## Predefined Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKqMVduAyJYA"
      },
      "source": [
        "### Adjacency Matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zs9snfcSyJYB"
      },
      "outputs": [],
      "source": [
        "def Adj(D, L, sparse=False):\n",
        "    N = L**D\n",
        "\n",
        "    # create all nodes' coordinates\n",
        "    nodes = [x for x in np.ndindex(tuple(np.repeat(L,D)))]\n",
        "\n",
        "    # Pass from coordinate to node's index\n",
        "    # (h,...k,j,i) <=> index = h*L^(D-1) + ... + k*L^2 + j*L + i\n",
        "    mul = [L**i for i in reversed(range(D))]\n",
        "\n",
        "    # Creation of adjacency matrix \n",
        "    A_dense = []\n",
        "    # creation of a row for each node's coordinate \n",
        "    for node in nodes:       \n",
        "        temp_buffer = []\n",
        "        A_dense_row = [0]*N\n",
        "        # find the two nearest neighbours of the node along each dimension\n",
        "        for d in range(D):\n",
        "            temp=list(node)\n",
        "            temp[d]=((temp[d]+1)%L)\n",
        "            temp=np.inner(temp, mul)\n",
        "            temp_buffer.append(temp)    \n",
        "\n",
        "            temp=list(node)\n",
        "            temp[d]=((temp[d]-1)%L)\n",
        "            temp=np.inner(temp, mul)\n",
        "            temp_buffer.append(temp)\n",
        "      \n",
        "        temp_buffer=list(np.unique(np.array(temp_buffer), axis=0))   \n",
        "        for i in temp_buffer: A_dense_row[i]=1\n",
        "        A_dense.append(A_dense_row)\n",
        "    \n",
        "    # sparse=False => sparse adjacency matrix\n",
        "    # sparse=True => dense adjacency matrix\n",
        "    if sparse:\n",
        "        return sp_matrix_to_sp_tensor(np.array(A_dense))\n",
        "    else:\n",
        "        return np.array(A_dense)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj5_2iJOyJYE"
      },
      "source": [
        "### Interaction Matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFHBU3C6yJYF"
      },
      "outputs": [],
      "source": [
        "def J_inter(denseAdj):\n",
        "    N = denseAdj.shape[0]\n",
        "    sparseAdj = sp_matrix_to_sp_tensor(denseAdj)\n",
        "\n",
        "    # sparse adjacency matrix as a numpy array\n",
        "    edge=sparseAdj.indices.numpy()\n",
        "\n",
        "    # ordered numpy sparse adjacency matrix\n",
        "    un_edge=np.array([np.sort(i) for i in edge]) \n",
        "\n",
        "    # creation of the interaction array: (i,j) and (j,i) have the same Jij\n",
        "    inter=[]\n",
        "    for i in range(len(un_edge)):\n",
        "        equal=True\n",
        "        for j in range(i):\n",
        "            if np.array_equal(un_edge[i],un_edge[j]):\n",
        "                inter.append(inter[j])\n",
        "                equal=False\n",
        "                break\n",
        "        if equal: \n",
        "            inter.append(np.random.normal(0, 1))\n",
        "    \n",
        "    # creation of dense interaction matrix\n",
        "    inter_matrix = np.zeros((N,N))\n",
        "    counter = 0\n",
        "    for i, j in edge:\n",
        "        inter_matrix[i,j] = inter[counter]\n",
        "        counter += 1\n",
        "    return [np.array(inter).reshape(sparseAdj.indices.shape[0],1), inter_matrix.reshape((N,N,1))]\n",
        "    \n",
        "    # index of the returned list:\n",
        "    # 0 => interaction array\n",
        "    # 1 => interaction matrix (zero padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcYp--P9yJYH"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMEfE3YyyJYJ"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, N_graph, X, Y, A, E, **kwargs):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.N_graph = N_graph\n",
        "        self.A = A\n",
        "        self.E = E\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        mydataset = []\n",
        "        for i in range(self.N_graph):\n",
        "            # list of Graph objects that will be used as input in the BatchLoader\n",
        "            mydataset.append(\n",
        "                    Graph(x=self.X[i], a=self.A[i], e=self.E[i], y=self.Y[i])      \n",
        "                    )\n",
        "        return mydataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKMlL7aXyJYK"
      },
      "source": [
        "### Replay Memory Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwfwO3odyJYM"
      },
      "outputs": [],
      "source": [
        "# save n-step transitions (s_t; a_t; r_t,t+n; s_t+n) from the trajectory buffer\n",
        "\n",
        "def get_replay_memory(trajectory_buffer, replay_memory):\n",
        "    n = len(trajectory_buffer)\n",
        "\n",
        "    states = np.array([transition[0] for transition in trajectory_buffer])\n",
        "    actions = np.array([transition[1] for transition in trajectory_buffer])\n",
        "    rewards = np.array([transition[2] for transition in trajectory_buffer])\n",
        "    done = np.array([transition[4] for transition in trajectory_buffer])\n",
        "    inter_matrix = trajectory_buffer[0][-1]\n",
        "    dense_AdjMat = trajectory_buffer[0][-2]\n",
        "\n",
        "    cum_reward = np.cumsum(rewards)\n",
        "\n",
        "    # creating the replay memory buffer from the trajectory one\n",
        "    # => (starting state, action performed, cumulative reward after n step from the starting one, state after n step from the starting one, episode ended, dense adjacency matrix of the episode, interaction matrix of the episode)\n",
        "    replay_memory.append([states[0], actions[0], cum_reward[n-1], states[n-1], done[n-1], dense_AdjMat, inter_matrix])\n",
        "\n",
        "    return replay_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEY_CY7VyJYN"
      },
      "source": [
        "### Real Energy Minima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhjfrmwhyJYN"
      },
      "outputs": [],
      "source": [
        "@jit(nopython=True)\n",
        "def computeEnergy(state, edge, interaction):  \n",
        "    energy = 0\n",
        "    for i in range(len(edge)):\n",
        "        energy -= interaction[i][0]*state[edge[i][0]][0]*state[edge[i][1]][0]\n",
        "    return energy/2\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def EnergyMinima(states, inter, sparseAdj):\n",
        "    # find the minimum energy for a fixed J_ij configuration\n",
        "    energy_min = np.inf\n",
        "    for state in states:\n",
        "        state = np.array(state).reshape(N,1)\n",
        "        state_energy = computeEnergy(state, sparseAdj, inter)\n",
        "        if state_energy<energy_min: energy_min=state_energy\n",
        "    return energy_min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7GwjAGayJYO"
      },
      "source": [
        "## Environment "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBckJ6iuyJYP"
      },
      "outputs": [],
      "source": [
        "class SG_env(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, L, D):\n",
        "    # Initialize environment attributes\n",
        "    # - action_spec: action declaration (integer from 0 to N-1)\n",
        "    # - observation_spec: state of the system declaration\n",
        "    # - episode_ended: flag for the end of the episode (all spin down)\n",
        "    # - state: state of the system (1 and -1 array)\n",
        "    self.N = L**D\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=self.N-1, name='action')\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(self.N,D+1), dtype=np.int32, minimum=-1, maximum=self.N-1, name='observation')\n",
        "    self.sp_AdjMat = Adj(D, L, sparse=True)\n",
        "    self.dense_AdjMat = Adj(D, L, sparse=False)\n",
        "    list_J = J_inter(self.dense_AdjMat)\n",
        "    self.interaction = list_J[0]   \n",
        "    self.inter_matrix = list_J[1]\n",
        "    nodes = [x for x in np.ndindex(tuple(np.repeat(L,2)))]\n",
        "    self._state = np.append(np.append(np.ones(shape=(self.N,1)), [[node[0]] for node in nodes], axis=1), [[node[1]] for node in nodes], axis=1).astype(\"int32\")\n",
        "    self._episode_ended = False\n",
        "\n",
        "  def get_state(self):\n",
        "    return self._state\n",
        "\n",
        "  def show_N(self):\n",
        "    return self.N  \n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def show_dense_AdjMat(self):\n",
        "    return self.dense_AdjMat\n",
        "\n",
        "  def show_sp_AdjMat(self):\n",
        "    return self.sp_AdjMat\n",
        "\n",
        "  def show_interaction(self):\n",
        "    return self.interaction\n",
        "\n",
        "  def show_inter_matrix(self):\n",
        "    return self.inter_matrix\n",
        "\n",
        "  # True => All spins = -1, False => otherwise\n",
        "  def __all_spins_down(self):\n",
        "    return np.all(self._state[:,0]==-1)    \n",
        "\n",
        "  # Compute the reward of the chosen action \n",
        "  # reward = energy difference between consecutive states\n",
        "  # nns => nearest neighbours indexes\n",
        "  # nn_Js => nearest neighbours interactions' indexes\n",
        "  def computeReward(self, action):\n",
        "    nns = self.sp_AdjMat.indices[self.sp_AdjMat.indices[:,0]==action][:,1].numpy()\n",
        "    nn_Js = np.where(self.sp_AdjMat.indices[:,0]==action)[0]\n",
        "    nn_sum = 0\n",
        "    for i in range(len(nns)): nn_sum += self.interaction[nn_Js[i]]*self._state[nns[i],0]\n",
        "    reward = 2*nn_sum*self._state[action,0]\n",
        "    return reward[0]\n",
        "\n",
        "  # Compute the energy of the current state \n",
        "  def computeEnergy(self):\n",
        "    edge = self.sp_AdjMat.indices.numpy()\n",
        "    Nedge = len(edge)\n",
        "    energy = 0\n",
        "    for i in range(Nedge):\n",
        "        energy -= self.interaction[i][0]*self._state[edge[i][0]][0]*self._state[edge[i][1]][0]\n",
        "    return energy/2\n",
        "\n",
        "  # reset function: called when all spins are -1 => new episode\n",
        "  #                                              => all spins up (=1) and new interaction matrix (if needed)\n",
        "  def _reset(self):\n",
        "    self._state[:,0] = 1\n",
        "    #self.interaction = J_inter(self.dense_AdjMat)[0]    \n",
        "    #self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n",
        "    self._episode_ended = False\n",
        "    return ts.restart(np.array(self._state, dtype=np.int32))\n",
        "\n",
        "  # step function: describe the process of applying the action selected by the agent\n",
        "  # ts.restart, ts.transition and ts.termination return a timestep \n",
        "  # containing step_type, reward, discount and observation\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    if self.__all_spins_down():\n",
        "      self._episode_ended = True\n",
        "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==1):\n",
        "      self._state[action,0]=-1\n",
        "      rew = self.computeReward(action)\n",
        "      \n",
        "      if self.__all_spins_down():\n",
        "          self._episode_ended = True\n",
        "          return ts.termination(np.array(self._state, dtype=np.int32), reward=rew)\n",
        "      else:\n",
        "          return ts.transition(np.array(self._state, dtype=np.int32), reward=rew)\n",
        "    \n",
        "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==-1):\n",
        "      raise ValueError('Each spin can be flipped only once!')\n",
        "    else:\n",
        "      raise ValueError('`action` should be 0 up to N-1 - Spin Flip!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGsbfJ__yJYQ"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rtWCa2syJYR"
      },
      "outputs": [],
      "source": [
        "# Agent (=> GNN+FNN) \n",
        "# N => number nodes\n",
        "# D => number dimensions\n",
        "# stack_channels => integer or list of integers, number of channels for the hidden layers\n",
        "# node_channels => integer, number of output channels for the nodes\n",
        "# edge_channels => integer, number of output channels for the edges\n",
        "# division_factor_dense => integer, gradually reduce the number of neurons for each dense layer\n",
        "# p_drop => float between 0 and 1, fraction of the input units to drop\n",
        "# N_xenet => Number of XENetDenseConv layers\n",
        "# N_dense => Number of Dense hidden layers\n",
        "\n",
        "def agent(N,                                 \n",
        "          D,                                 \n",
        "          stack_channels=5,        \n",
        "          node_channels=3,\n",
        "          edge_channels=3,\n",
        "          division_factor_dense=4,\n",
        "          p_drop=0,\n",
        "          N_xenet=2,\n",
        "          N_dense=2,\n",
        "          activation=\"relu\",\n",
        "          regularizer=0):\n",
        "  inX = Input(shape=(N,D+1), name='Input Nodes')\n",
        "  inA = Input(shape=(N,N), name='Input Adj matrix')\n",
        "  inE = Input(shape=(N,N,1), name='Input Edges')\n",
        "  \n",
        "  X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
        "                               attention=True, node_activation=activation, edge_activation=activation, \n",
        "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_0\")([inX, inA, inE])\n",
        "  for i in range(N_xenet-1):\n",
        "    X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
        "                               attention=True, node_activation=activation, edge_activation=activation, \n",
        "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_\"+str(i+1))([X, inA, E])\n",
        "  \n",
        "  # flat the updated X, E in order to feed the fully connected neural network (FNN)\n",
        "  flat_x, flat_e = Flatten(name=\"Nodes_encoding\")(X), Flatten(name=\"Edges_encoding\")(E)\n",
        "  out = Concatenate(axis=1, name=\"Concatenation\")([flat_x])    #,flat_e\n",
        "  \n",
        "  for i in range(N_dense):\n",
        "    out = Dense(out.shape.as_list()[1]//division_factor_dense, activation=activation, kernel_regularizer=l2(regularizer))(out)\n",
        "    out = Dropout(p_drop)(out)\n",
        "  out = Dense(N, activation=\"PReLU\", kernel_regularizer=l2(regularizer), name='Q-values')(out)\n",
        "  \n",
        "  model = Model([inX,inA,inE], out)\n",
        "  model.compile(optimizer=Adam(), loss=MeanSquaredError())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O_14QuiyJYS"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whlinbUayJYT"
      },
      "outputs": [],
      "source": [
        "def train(env, replay_memory, model, target_model, done):\n",
        "    discount_factor = 0.618\n",
        "    \n",
        "    # skip the training if the number of samples in the replay \n",
        "    # memory is less than MIN_REPLAY_SIZE\n",
        "    MIN_REPLAY_SIZE = 100\n",
        "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
        "        return\n",
        "\n",
        "    # randomly select a number of samples from the replay memory equal to batch_size\n",
        "    batch_size = 60\n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "\n",
        "    # sets of all interaction matrix and dense adjacency matrix in the batch\n",
        "    E = np.array([transition[-1] for transition in mini_batch])\n",
        "    A = np.array([transition[-2] for transition in mini_batch])\n",
        "\n",
        "    # current_states => set of all starting observations in the batch\n",
        "    # current_qs_list => predicted Q-values of the current_states by the model\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    current_qs_list = np.array(model.predict([current_states,A,E]))\n",
        " \n",
        "    # new_current_states => set of observations after performing n actions in the batch\n",
        "    # future_qs_list => predicted Q-values of the new_current_states by the target model\n",
        "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
        "    future_qs_list = np.array(target_model.predict([new_current_states,A,E]))\n",
        "    \n",
        "    # X => observations\n",
        "    # Y => 'label' of each observation: updated current_qs_list \n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (observation, action, reward, new_observation, done, dense_AdjMat, inter_matrix) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            new_q = reward + discount_factor*np.max(future_qs_list[index])\n",
        "        else:\n",
        "            new_q = reward\n",
        "\n",
        "        # update the Q-value corrisponding to the performed action\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = new_q\n",
        "        \n",
        "        X.append(observation)\n",
        "        Y.append(current_qs)\n",
        "\n",
        "    # build the training dataset \n",
        "    train_data = MyDataset(N_graph=batch_size, X=X, Y=Y, A=A, E=E)\n",
        "    # use the BatchLoader to fit the model and WandbCallback to load the loss to Weight&Biases\n",
        "    loader = BatchLoader(train_data, node_level=False, epochs=50, batch_size=batch_size, shuffle=False) \n",
        "    model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, verbose=2,\n",
        "                callbacks = [WandbCallback()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = agent(N=36, D=2, stack_channels=73, node_channels=40, edge_channels=40,\n",
        "                  N_xenet=3, N_dense=3, division_factor_dense=2)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJZSsbp9RR2b",
        "outputId": "fa5f34fd-330b-463e-d4d2-350a49a3c013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input Nodes (InputLayer)       [(None, 36, 3)]      0           []                               \n",
            "                                                                                                  \n",
            " Input Adj matrix (InputLayer)  [(None, 36, 36)]     0           []                               \n",
            "                                                                                                  \n",
            " Input Edges (InputLayer)       [(None, 36, 36, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " XENet_layer_0 (XENetDenseConv)  ((None, 36, 40),    9838        ['Input Nodes[0][0]',            \n",
            "                                 (None, 36, 36, 40)               'Input Adj matrix[0][0]',       \n",
            "                                )                                 'Input Edges[0][0]']            \n",
            "                                                                                                  \n",
            " XENet_layer_1 (XENetDenseConv)  ((None, 36, 40),    22414       ['XENet_layer_0[0][0]',          \n",
            "                                 (None, 36, 36, 40)               'Input Adj matrix[0][0]',       \n",
            "                                )                                 'XENet_layer_0[0][1]']          \n",
            "                                                                                                  \n",
            " XENet_layer_2 (XENetDenseConv)  ((None, 36, 40),    22414       ['XENet_layer_1[0][0]',          \n",
            "                                 (None, 36, 36, 40)               'Input Adj matrix[0][0]',       \n",
            "                                )                                 'XENet_layer_1[0][1]']          \n",
            "                                                                                                  \n",
            " Nodes_encoding (Flatten)       (None, 1440)         0           ['XENet_layer_2[0][0]']          \n",
            "                                                                                                  \n",
            " Concatenation (Concatenate)    (None, 1440)         0           ['Nodes_encoding[0][0]']         \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 720)          1037520     ['Concatenation[0][0]']          \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 720)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 360)          259560      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 360)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 180)          64980       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 180)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " Q-values (Dense)               (None, 36)           6552        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,423,278\n",
            "Trainable params: 1,423,278\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOReIikvyJYU"
      },
      "outputs": [],
      "source": [
        "def main(env): \n",
        "    \n",
        "    # Epsilon-greedy algorithm in initialized at 1 \n",
        "    # => every step is random at the start\n",
        "    epsilon = 1          \n",
        "    max_epsilon = 1     \n",
        "    min_epsilon = 0.01  \n",
        "    decay = 0.05\n",
        "\n",
        "    # 1. Initialize the Target and Main models \n",
        "    # Main Model (updated every 3 steps)\n",
        "    model = agent(N=env.N, D=D, stack_channels=73, node_channels=40, edge_channels=40,\n",
        "                  N_xenet=3, N_dense=3, division_factor_dense=2)\n",
        "\n",
        "    # Target Model (updated at the end of every episode)\n",
        "    target_model = agent(N=env.N, D=D, stack_channels=73, node_channels=40, edge_channels=40,\n",
        "                         N_xenet=3, N_dense=3, division_factor_dense=2)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "    energy_buffer = []\n",
        "    replay_memory = []\n",
        "    \n",
        "    step_buffer = 12\n",
        "    set_target = 1\n",
        "    train_episodes = 130\n",
        "    for episode in range(train_episodes):\n",
        "        # reset the variables at the beginning of an episode\n",
        "        trajectory_buffer = []   \n",
        "        ep_min_energy = np.inf\n",
        "        steps_to_update_target_model = 0\n",
        "        env.reset()\n",
        "        previous_obs = env.get_state()\n",
        "        done = False  \n",
        "        check = np.arange(0,env.N)\n",
        "\n",
        "        while not done: \n",
        "            observation = env.get_state()          \n",
        "            print(\"\\n\\n\\t\\t\\t\\t++++++++++++  episode:\", episode,\" - step:\", steps_to_update_target_model, \" ++++++++++++\")\n",
        "\n",
        "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "            random_number = np.random.rand()\n",
        "            if random_number <= epsilon:\n",
        "                # Explore\n",
        "                action = random.choice(check)\n",
        "\n",
        "            else:\n",
        "                # Exploit best known action\n",
        "                predicted = model([observation.reshape(1,env.N,3), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n",
        "                while True:\n",
        "                    # check to prevent flipping the same spin twice - only once!\n",
        "                    action = np.argmax(predicted)\n",
        "                    if env.get_state()[action,0] == 1:\n",
        "                            break;\n",
        "                    predicted[action] = np.NINF\n",
        "\n",
        "            # remove the choosen action from check array\n",
        "            check = np.setdiff1d(check, action)\n",
        "\n",
        "            # perform the action on the environment and get the updated parameters\n",
        "            step_type, reward, discount, new_observation = env._step(action)\n",
        "            done = env._episode_ended\n",
        "            e = env.computeEnergy()\n",
        "\n",
        "            # save the parameter in the buffer\n",
        "            trajectory_buffer.append([previous_obs, action, reward, new_observation, done, e, env.dense_AdjMat, env.inter_matrix])\n",
        "            energy_buffer.append([episode, new_observation, env.interaction, e])  \n",
        "            if ep_min_energy>e: ep_min_energy = e\n",
        "            \n",
        "            # load interesting parameters to weight&Biases\n",
        "            wandb.log({\n",
        "                \"Episode\": energy_buffer[episode*env.N+steps_to_update_target_model][0],\n",
        "                \"Step\": episode*env.N+steps_to_update_target_model,\n",
        "                #\"New observation\": wandb.Image(energy_buffer[episode*env.N+steps_to_update_target_model][1].reshape(L,L)),\n",
        "                \"J interactions\": energy_buffer[episode*env.N+steps_to_update_target_model][2],\n",
        "                \"Energy\": energy_buffer[episode*env.N+steps_to_update_target_model][3]\n",
        "            })\n",
        "\n",
        "            # fill the replay memory buffer\n",
        "            if steps_to_update_target_model >= step_buffer:   \n",
        "                replay_memory = get_replay_memory(trajectory_buffer, replay_memory)\n",
        "                trajectory_buffer = trajectory_buffer[1:]\n",
        "\n",
        "            # 3. Update the Main Network using the Bellman Equation  \n",
        "            #if (steps_to_update_target_model%L==0 and steps_to_update_target_model!=0) or done:\n",
        "            print(\"\\n\\t\\t\\t\\t\\t      +++++ Training +++++\")\n",
        "            train(env, replay_memory, model, target_model, done)\n",
        "  \n",
        "            previous_obs = new_observation\n",
        "\n",
        "            # Copying main network weights to the target network\n",
        "            # weights at the end of the episode\n",
        "            if done:\n",
        "                if episode >= set_target:\n",
        "                    target_model.set_weights(model.get_weights())\n",
        "                break\n",
        "\n",
        "            steps_to_update_target_model += 1\n",
        "\n",
        "        # update epsilon using the following rule\n",
        "        epsilon = min_epsilon + (max_epsilon -min_epsilon) * np.exp(-decay *episode)\n",
        "        wandb.log({\"Episode Energy Minima\" : ep_min_energy})\n",
        "\n",
        "    return target_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y02vL-NWyJYV"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot4sDW0oyJYW"
      },
      "outputs": [],
      "source": [
        "def test(env, model, L): \n",
        "    env.reset()\n",
        "    energy = []\n",
        "    done = False\n",
        "\n",
        "    while not done: \n",
        "        observation = env.get_state()          \n",
        "        wandb.log({\"Test state\": wandb.Image(observation.reshape(L,L))})\n",
        "        \n",
        "        # Exploit best known action\n",
        "        predicted = model([observation.reshape(1,env.N,1), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n",
        "        while True:\n",
        "            #check to prevent flipping the same spin twice - only once!\n",
        "            action = np.argmax(predicted)\n",
        "            if env.get_state()[action,0] == 1:\n",
        "                    break;\n",
        "            predicted[action] = np.NINF\n",
        "\n",
        "        step_type, reward, discount, new_observation = env._step(action) \n",
        "        done = env._episode_ended\n",
        "        e = env.computeEnergy()\n",
        "        energy.append([new_observation, e])\n",
        "        \n",
        "    ground_state = np.min([e[1] for e in energy])         \n",
        "    return ground_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FT_4ho0yJYW"
      },
      "source": [
        "# GO!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrbs75u_yJYX"
      },
      "outputs": [],
      "source": [
        "L = 6\n",
        "D = 2\n",
        "N = L**D   \n",
        "environment = SG_env(L=L, D=D)\n",
        "\n",
        "# train and save the model\n",
        "#trained_targetModel = main(environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2WT3z_qyJYY"
      },
      "outputs": [],
      "source": [
        "# create a list of all possible states\n",
        "ensemble = [1, -1]\n",
        "all_states = [x for x in product(ensemble, repeat=N)]\n",
        "# compute the true energy minima\n",
        "trueEnergy = EnergyMinima(states=all_states, inter=environment.interaction, sparseAdj=environment.sp_AdjMat.indices.numpy())\n",
        "print(\"\\n\\nTrue energy minima\\n =>\", trueEnergy)\n",
        "\n",
        "# test the trained model\n",
        "foundEnergy = test(environment, trained_targetModel, L=L)\n",
        "print(\"\\nFound energy minima\\n =>\", foundEnergy, \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMHz_eA4yJYZ"
      },
      "outputs": [],
      "source": [
        "wandb.log({\n",
        "    \"True Energy\"  : trueEnergy,\n",
        "    \"Found Energy\" : foundEnergy\n",
        "})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY3lG0TpyJYZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "XFeatures.ipynb",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "61c4312e-360b-4641-8d4d-1b368b7458c8",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}