{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "8abe6b57-b498-49fa-abc1-938c46fb211d",
    "deepnote_cell_height": 549,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16126,
    "execution_start": 1654448721151,
    "source_hash": "9c0d0462",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:38:08.346339: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-09 11:38:08.346398: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from itertools import product\n",
    "from wandb.keras import WandbCallback\n",
    "from numba import jit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Input, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, Huber, MeanSquaredError\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "\n",
    "from spektral.layers import XENetDenseConv\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.data import Graph\n",
    "from spektral.data.dataset import Dataset\n",
    "from spektral.data.loaders import BatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "5591f48e7014481f81da80c576075999",
    "deepnote_cell_height": 262.375,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13761,
    "execution_start": 1654448978592,
    "owner_user_id": "d8aa7146-c8e6-479e-9e49-136a058e4f85",
    "source_hash": "538d9111",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "2022-06-09 11:38:26.159462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-09 11:38:26.159503: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/opt/conda/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:47: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/workspace/wandb/run-20220609_113824-12qwk2fe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/locp/Test%20-%20D%3D2%20%26%20L%3D6/runs/12qwk2fe\" target=\"_blank\">ethereal-sound-4</a></strong> to <a href=\"https://wandb.ai/locp/Test%20-%20D%3D2%20%26%20L%3D6\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# experiment configuration saved to W&B\n",
    "config_defaults = {\n",
    "  \"N_xenet\" : 1,\n",
    "  \"N_dense\" : 17, \n",
    "  \"division_factor_dense\" : 1,\n",
    "  \"stack_channels\" : 73,\n",
    "  \"node_channels\" : 20,\n",
    "  \"edge_channels\": 20,\n",
    "  \"train_episodes\" : 5,         #300,\n",
    "  \"MIN_REPLAY_SIZE\" : 100, \n",
    "  \"batch_size\": 60,\n",
    "  \"step_buffer\": 15,\n",
    "  \"set_target\": 9,\n",
    "  \"decay\": 0.01,\n",
    "  \"discount_factor\": 0.618,\n",
    "}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(project=\"Test - D=2 & L=6\", \n",
    "           entity=\"locp\", config=config_defaults)\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "764f0144705b477b85c29b6f5e45277c",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Predefined Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c54bacdda82b4818995e6a46948e40d2",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Adjacency Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "d422ff0c4fe74fe4a0a8ba07d42b05dd",
    "deepnote_cell_height": 732,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "63f6794e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Adj(D, L, sparse=False):\n",
    "    N = L**D\n",
    "\n",
    "    # create all nodes' coordinates\n",
    "    nodes = [x for x in np.ndindex(tuple(np.repeat(L,D)))]\n",
    "\n",
    "    # Pass from coordinate to node's index\n",
    "    # (h,...k,j,i) <=> index = h*L^(D-1) + ... + k*L^2 + j*L + i\n",
    "    mul = [L**i for i in reversed(range(D))]\n",
    "\n",
    "    # Creation of adjacency matrix \n",
    "    A_dense = []\n",
    "    # creation of a row for each node's coordinate \n",
    "    for node in nodes:       \n",
    "        temp_buffer = []\n",
    "        A_dense_row = [0]*N\n",
    "        # find the two nearest neighbours of the node along each dimension\n",
    "        for d in range(D):\n",
    "            temp=list(node)\n",
    "            temp[d]=((temp[d]+1)%L)\n",
    "            temp=np.inner(temp, mul)\n",
    "            temp_buffer.append(temp)    \n",
    "\n",
    "            temp=list(node)\n",
    "            temp[d]=((temp[d]-1)%L)\n",
    "            temp=np.inner(temp, mul)\n",
    "            temp_buffer.append(temp)\n",
    "      \n",
    "        temp_buffer=list(np.unique(np.array(temp_buffer), axis=0))   \n",
    "        for i in temp_buffer: A_dense_row[i]=1\n",
    "        A_dense.append(A_dense_row)\n",
    "    \n",
    "    # sparse=False => sparse adjacency matrix\n",
    "    # sparse=True => dense adjacency matrix\n",
    "    if sparse:\n",
    "        return sp_matrix_to_sp_tensor(np.array(A_dense))\n",
    "    else:\n",
    "        return np.array(A_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9633863cbc994b2eaf1281ad3f95844c",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Interaction Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "6f04bacceeba47cf9ea0943970aa23c6",
    "deepnote_cell_height": 642,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "3c2729e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def J_inter(denseAdj):\n",
    "    N = denseAdj.shape[0]\n",
    "    sparseAdj = sp_matrix_to_sp_tensor(denseAdj)\n",
    "\n",
    "    # sparse adjacency matrix as a numpy array\n",
    "    edge=sparseAdj.indices.numpy()\n",
    "\n",
    "    # ordered numpy sparse adjacency matrix\n",
    "    un_edge=np.array([np.sort(i) for i in edge]) \n",
    "\n",
    "    # creation of the interaction array: (i,j) and (j,i) have the same Jij\n",
    "    inter=[]\n",
    "    for i in range(len(un_edge)):\n",
    "        equal=True\n",
    "        for j in range(i):\n",
    "            if np.array_equal(un_edge[i],un_edge[j]):\n",
    "                inter.append(inter[j])\n",
    "                equal=False\n",
    "                break\n",
    "        if equal: \n",
    "            inter.append(np.random.normal(0, 1))\n",
    "    \n",
    "    # creation of dense interaction matrix\n",
    "    inter_matrix = np.zeros((N,N))\n",
    "    counter = 0\n",
    "    for i, j in edge:\n",
    "        inter_matrix[i,j] = inter[counter]\n",
    "        counter += 1\n",
    "    return [np.array(inter).reshape(sparseAdj.indices.shape[0],1), inter_matrix.reshape((N,N,1))]\n",
    "    \n",
    "    # index of the returned list:\n",
    "    # 0 => interaction array\n",
    "    # 1 => interaction matrix (zero padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "150d8615cc1e4d9cbb9c6e2eec84b103",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "1dd5d3a30c254ba489255b41c481d690",
    "deepnote_cell_height": 354,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5630b81b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, N_graph, X, Y, A, E, **kwargs):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.N_graph = N_graph\n",
    "        self.A = A\n",
    "        self.E = E\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        mydataset = []\n",
    "        for i in range(self.N_graph):\n",
    "            # list of Graph objects that will be used as input in the BatchLoader\n",
    "            mydataset.append(\n",
    "                    Graph(x=self.X[i], a=self.A[i], e=self.E[i], y=self.Y[i])      \n",
    "                    )\n",
    "        return mydataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d5be6a8dc114757861855415712f148",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Replay Memory Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "e181f4f5e9b845cc94a00c162e8e16fc",
    "deepnote_cell_height": 390,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8ae44cb4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save n-step transitions (s_t; a_t; r_t,t+n; s_t+n) from the trajectory buffer\n",
    "\n",
    "def get_replay_memory(trajectory_buffer, replay_memory):\n",
    "    n = len(trajectory_buffer)\n",
    "\n",
    "    states = np.array([transition[0] for transition in trajectory_buffer])\n",
    "    actions = np.array([transition[1] for transition in trajectory_buffer])\n",
    "    rewards = np.array([transition[2] for transition in trajectory_buffer])\n",
    "    done = np.array([transition[4] for transition in trajectory_buffer])\n",
    "    inter_matrix = trajectory_buffer[0][-1]\n",
    "    dense_AdjMat = trajectory_buffer[0][-2]\n",
    "\n",
    "    cum_reward = np.cumsum(rewards)\n",
    "\n",
    "    # creating the replay memory buffer from the trajectory one\n",
    "    # => (starting state, action performed, cumulative reward after n step from the starting one, state after n step from the starting one, episode ended, dense adjacency matrix of the episode, interaction matrix of the episode)\n",
    "    replay_memory.append([states[0], actions[0], cum_reward[n-1], states[n-1], done[n-1], dense_AdjMat, inter_matrix])\n",
    "\n",
    "    return replay_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3b9dca89ae1e45a7a63ded986aa70996",
    "deepnote_cell_height": 62,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Real Energy Minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "c0e39cae441b46248073b27d9d63221c",
    "deepnote_cell_height": 480,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "712751ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def computeEnergy(state, edge, interaction):  \n",
    "    energy = 0\n",
    "    for i in range(len(edge)):\n",
    "        energy -= interaction[i][0]*state[edge[i][0]][0]*state[edge[i][1]][0]\n",
    "    return energy/2\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def EnergyMinima(states, inter, sparseAdj):\n",
    "    # find the minimum energy for a fixed J_ij configuration\n",
    "    energy_min = np.inf\n",
    "    for state in states:\n",
    "        state = np.array(state).reshape(N,1)\n",
    "        state_energy = computeEnergy(state, sparseAdj, inter)\n",
    "        if state_energy<energy_min: energy_min=state_energy\n",
    "    return energy_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e600cf72362489c987cfce6420ba23c",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "86b0cc302b154363947a634d2d672daa",
    "deepnote_cell_height": 1866,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "bf0558b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SG_env(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self, L, D):\n",
    "    # Initialize environment attributes\n",
    "    # - action_spec: action declaration (integer from 0 to N-1)\n",
    "    # - observation_spec: state of the system declaration\n",
    "    # - episode_ended: flag for the end of the episode (all spin down)\n",
    "    # - state: state of the system (1 and -1 array)\n",
    "    self.N = L**D\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(), dtype=np.int32, minimum=0, maximum=self.N-1, name='action')\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(self.N,1), dtype=np.int32, minimum=-1, maximum=1, name='observation')\n",
    "    self.sp_AdjMat = Adj(D, L, sparse=True)\n",
    "    self.dense_AdjMat = Adj(D, L, sparse=False)\n",
    "    self.interaction = J_inter(self.dense_AdjMat)[0]   \n",
    "    self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n",
    "    self._state = np.ones(shape=(self.N,1)).astype(\"int32\")\n",
    "    self._episode_ended = False\n",
    "\n",
    "  def get_state(self):\n",
    "    return self._state\n",
    "\n",
    "  def show_N(self):\n",
    "    return self.N  \n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def show_dense_AdjMat(self):\n",
    "    return self.dense_AdjMat\n",
    "\n",
    "  def show_sp_AdjMat(self):\n",
    "    return self.sp_AdjMat\n",
    "\n",
    "  def show_interaction(self):\n",
    "    return self.interaction\n",
    "\n",
    "  def show_inter_matrix(self):\n",
    "    return self.inter_matrix\n",
    "\n",
    "  # True => All spins = -1, False => otherwise\n",
    "  def __all_spins_down(self):\n",
    "    return np.all(self._state==-1)    \n",
    "\n",
    "  # Compute the reward of the chosen action \n",
    "  # reward = energy difference between consecutive states\n",
    "  # nns => nearest neighbours indexes\n",
    "  # nn_Js => nearest neighbours interactions' indexes\n",
    "  def computeReward(self, action):\n",
    "    nns = self.sp_AdjMat.indices[self.sp_AdjMat.indices[:,0]==action][:,1].numpy()\n",
    "    nn_Js = np.where(self.sp_AdjMat.indices[:,0]==action)[0]\n",
    "    nn_sum = 0\n",
    "    for i in range(len(nns)): nn_sum += self.interaction[nn_Js[i]]*self._state[nns[i],0]\n",
    "    reward = 2*nn_sum*self._state[action,0]\n",
    "    return reward[0]\n",
    "\n",
    "  # Compute the energy of the current state \n",
    "  def computeEnergy(self):\n",
    "    edge = self.sp_AdjMat.indices.numpy()\n",
    "    Nedge = len(edge)\n",
    "    energy = 0\n",
    "    for i in range(Nedge):\n",
    "        energy -= self.interaction[i][0]*self._state[edge[i][0]][0]*self._state[edge[i][1]][0]\n",
    "    return energy/2\n",
    "\n",
    "  # reset function: called when all spins are -1 => new episode\n",
    "  #                                              => all spins up (=1) and new interaction matrix (if needed)\n",
    "  def _reset(self):\n",
    "    self._state = np.ones(shape=(self.N,1)).astype(\"int32\")\n",
    "    #self.interaction = J_inter(self.dense_AdjMat)[0]    \n",
    "    #self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n",
    "    self._episode_ended = False\n",
    "    return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "  # step function: describe the process of applying the action selected by the agent\n",
    "  # ts.restart, ts.transition and ts.termination return a timestep \n",
    "  # containing step_type, reward, discount and observation\n",
    "  def _step(self, action):\n",
    "    if self._episode_ended:\n",
    "      return self.reset()\n",
    "\n",
    "    if self.__all_spins_down():\n",
    "      self._episode_ended = True\n",
    "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==1):\n",
    "      self._state[action,0]=-1\n",
    "      rew = self.computeReward(action)\n",
    "      \n",
    "      if self.__all_spins_down():\n",
    "          self._episode_ended = True\n",
    "          return ts.termination(np.array(self._state, dtype=np.int32), reward=rew)\n",
    "      else:\n",
    "          return ts.transition(np.array(self._state, dtype=np.int32), reward=rew)\n",
    "    \n",
    "    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==-1):\n",
    "      raise ValueError('Each spin can be flipped only once!')\n",
    "    else:\n",
    "      raise ValueError('`action` should be 0 up to N-1 - Spin Flip!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2b00da234ac54593b0738d6452581547",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "6ef186888fa04be09638f4774e61a823",
    "deepnote_cell_height": 876,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "922c1210",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agent (=> GNN+FNN) \n",
    "# N => number nodes\n",
    "# D => number dimensions\n",
    "# stack_channels => integer or list of integers, number of channels for the hidden layers\n",
    "# node_channels => integer, number of output channels for the nodes\n",
    "# edge_channels => integer, number of output channels for the edges\n",
    "# division_factor_dense => integer, gradually reduce the number of neurons for each dense layer\n",
    "# p_drop => float between 0 and 1, fraction of the input units to drop\n",
    "# N_xenet => Number of XENetDenseConv layers\n",
    "# N_dense => Number of Dense hidden layers\n",
    "\n",
    "def agent(N,                                 \n",
    "          D,                                 \n",
    "          stack_channels=5,        \n",
    "          node_channels=3,\n",
    "          edge_channels=3,\n",
    "          division_factor_dense=4,\n",
    "          p_drop=0,\n",
    "          N_xenet=2,\n",
    "          N_dense=2,\n",
    "          activation=\"relu\",\n",
    "          regularizer=0):\n",
    "  inX = Input(shape=(N,1), name='Input Nodes')\n",
    "  inA = Input(shape=(N,N), name='Input Adj matrix')\n",
    "  inE = Input(shape=(N,N,1), name='Input Edges')\n",
    "  \n",
    "  X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
    "                               attention=True, node_activation=activation, edge_activation=activation, \n",
    "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_0\")([inX, inA, inE])\n",
    "  for i in range(N_xenet-1):\n",
    "    X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n",
    "                               attention=True, node_activation=activation, edge_activation=activation, \n",
    "                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_\"+str(i+1))([X, inA, E])\n",
    "  \n",
    "  # flat the updated X, E in order to feed the fully connected neural network (FNN)\n",
    "  flat_x, flat_e = Flatten(name=\"Nodes_encoding\")(X), Flatten(name=\"Edges_encoding\")(E)\n",
    "  out = Concatenate(axis=1, name=\"Concatenation\")([flat_x])    #,flat_e\n",
    "  \n",
    "  for i in range(N_dense):\n",
    "    out = Dense(out.shape.as_list()[1]//division_factor_dense, activation=activation, kernel_regularizer=l2(regularizer))(out)\n",
    "    out = Dropout(p_drop)(out)\n",
    "  out = Dense(N, activation=\"PReLU\", kernel_regularizer=l2(regularizer), name='Q-values')(out)\n",
    "  \n",
    "  model = Model([inX,inA,inE], out)\n",
    "  model.compile(optimizer=Adam(), loss=MeanSquaredError())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5713c3657aac41e4a3b3cd276685c9b2",
    "deepnote_cell_height": 70,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "5624f1c5022a4d66b1c68d1a7075a781",
    "deepnote_cell_height": 948,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9b2c6fac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(env, replay_memory, model, target_model, done):    \n",
    "    # skip the training if the number of samples in the replay \n",
    "    # memory is less than MIN_REPLAY_SIZE\n",
    "    if len(replay_memory) < cfg.MIN_REPLAY_SIZE:\n",
    "        return\n",
    "\n",
    "    # randomly select a number of samples from the replay memory equal to batch_size\n",
    "    batch_size = cfg.batch_size\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "\n",
    "    # sets of all interaction matrix and dense adjacency matrix in the batch\n",
    "    E = np.array([transition[-1] for transition in mini_batch])\n",
    "    A = np.array([transition[-2] for transition in mini_batch])\n",
    "\n",
    "    # current_states => set of all starting observations in the batch\n",
    "    # current_qs_list => predicted Q-values of the current_states by the model\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "    current_qs_list = np.array(model.predict([current_states,A,E]))\n",
    " \n",
    "    # new_current_states => set of observations after performing n actions in the batch\n",
    "    # future_qs_list => predicted Q-values of the new_current_states by the target model\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "    future_qs_list = np.array(target_model.predict([new_current_states,A,E]))\n",
    "    \n",
    "    # X => observations\n",
    "    # Y => 'label' of each observation: updated current_qs_list \n",
    "    X = []\n",
    "    Y = []\n",
    "    discount_factor = cfg.discount_factor\n",
    "    for index, (observation, action, reward, new_observation, done, dense_AdjMat, inter_matrix) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            new_q = reward + discount_factor*np.max(future_qs_list[index])\n",
    "        else:\n",
    "            new_q = reward\n",
    "\n",
    "        # update the Q-value corrisponding to the performed action\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = new_q\n",
    "        \n",
    "        X.append(observation)\n",
    "        Y.append(current_qs)\n",
    "\n",
    "    # build the training dataset \n",
    "    train_data = MyDataset(N_graph=batch_size, X=X, Y=Y, A=A, E=E)\n",
    "    # use the BatchLoader to fit the model and WandbCallback to load the loss to Weight&Biases\n",
    "    loader = BatchLoader(train_data, node_level=False, epochs=50, batch_size=batch_size, shuffle=False) \n",
    "    model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, verbose=2,\n",
    "                callbacks = [WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "2257d42bd8024c788fd22c3e04e7c7b1",
    "deepnote_cell_height": 1902,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8a8b0363",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(env): \n",
    "    \n",
    "    # Epsilon-greedy algorithm in initialized at 1 \n",
    "    # => every step is random at the start\n",
    "    epsilon = 1          \n",
    "    max_epsilon = 1     \n",
    "    min_epsilon = 0.01  \n",
    "    decay = cfg.decay\n",
    "\n",
    "    # 1. Initialize the Target and Main models \n",
    "    # Main Model (updated every 3 steps)\n",
    "    model = agent(N=env.N, D=D, stack_channels=cfg.stack_channels, node_channels=cfg.node_channels, \n",
    "                  edge_channels=cfg.edge_channels, N_xenet=cfg.N_xenet, N_dense=cfg.N_dense, \n",
    "                  division_factor_dense=cfg.division_factor_dense)\n",
    "\n",
    "    # Target Model (updated at the end of every episode)\n",
    "    target_model = agent(N=env.N, D=D, stack_channels=cfg.stack_channels, node_channels=cfg.node_channels, \n",
    "                         edge_channels=cfg.edge_channels, N_xenet=cfg.N_xenet, N_dense=cfg.N_dense, \n",
    "                         division_factor_dense=cfg.division_factor_dense)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    energy_buffer = []\n",
    "    replay_memory = []\n",
    "    \n",
    "    # custom plot parameters\n",
    "    ep_ene_min_data = []\n",
    "    step_ene_min_data = []    \n",
    "    \n",
    "    for episode in range(cfg.train_episodes):\n",
    "        # reset the variables at the beginning of an episode\n",
    "        trajectory_buffer = []   \n",
    "        ep_min_energy = np.inf\n",
    "        steps_to_update_target_model = 0\n",
    "        env.reset()\n",
    "        previous_obs = np.ones(shape=(env.N,1)).astype(\"int32\")\n",
    "        done = False  \n",
    "        check = np.arange(0,env.N)\n",
    "\n",
    "        while not done: \n",
    "            observation = env.get_state()          \n",
    "            print(\"\\n\\n\\t\\t\\t\\t++++++++++++  episode:\", episode,\" - step:\", steps_to_update_target_model, \" ++++++++++++\")\n",
    "\n",
    "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "            random_number = np.random.rand()\n",
    "            if random_number <= epsilon:\n",
    "                # Explore\n",
    "                action = random.choice(check)\n",
    "\n",
    "            else:\n",
    "                # Exploit best known action\n",
    "                predicted = model([observation.reshape(1,env.N,1), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n",
    "                while True:\n",
    "                    # check to prevent flipping the same spin twice - only once!\n",
    "                    action = np.argmax(predicted)\n",
    "                    if env.get_state()[action,0] == 1:\n",
    "                            break;\n",
    "                    predicted[action] = np.NINF\n",
    "\n",
    "            # remove the choosen action from check array\n",
    "            check = np.setdiff1d(check, action)\n",
    "\n",
    "            # perform the action on the environment and get the updated parameters\n",
    "            step_type, reward, discount, new_observation = env._step(action)\n",
    "            done = env._episode_ended\n",
    "            e = env.computeEnergy()\n",
    "\n",
    "            # save the parameter in the buffer\n",
    "            trajectory_buffer.append([previous_obs, action, reward, new_observation, done, e, env.dense_AdjMat, env.inter_matrix])\n",
    "            energy_buffer.append([episode, new_observation, env.interaction, e])  \n",
    "            if ep_min_energy>e: ep_min_energy = e\n",
    "            \n",
    "            # load interesting parameters to weight&Biases\n",
    "            step_ene_min_data.append([e, episode*env.N+steps_to_update_target_model])    \n",
    "            wandb.log({\n",
    "                #\"Episode\": energy_buffer[episode*env.N+steps_to_update_target_model][0],\n",
    "                #\"Step\": episode*env.N+steps_to_update_target_model,\n",
    "                \"New observation\": wandb.Image(energy_buffer[episode*env.N+steps_to_update_target_model][1].reshape(L,L)),\n",
    "                \"J interactions\": energy_buffer[episode*env.N+steps_to_update_target_model][2],\n",
    "                # \"Energy\": energy_buffer[episode*env.N+steps_to_update_target_model][3]\n",
    "            })\n",
    "\n",
    "            # fill the replay memory buffer\n",
    "            if steps_to_update_target_model >= cfg.step_buffer:   \n",
    "                replay_memory = get_replay_memory(trajectory_buffer, replay_memory)\n",
    "                trajectory_buffer = trajectory_buffer[1:]\n",
    "\n",
    "            # 3. Update the Main Network using the Bellman Equation  \n",
    "            #if (steps_to_update_target_model%L==0 and steps_to_update_target_model!=0) or done:\n",
    "            print(\"\\n\\t\\t\\t\\t\\t      +++++ Training +++++\")\n",
    "            train(env, replay_memory, model, target_model, done)\n",
    "  \n",
    "            previous_obs = new_observation\n",
    "\n",
    "            # Copying main network weights to the target network\n",
    "            # weights at the end of the episode\n",
    "            if done:\n",
    "                if episode >= cfg.set_target:\n",
    "                    target_model.set_weights(model.get_weights())\n",
    "                break\n",
    "\n",
    "            steps_to_update_target_model += 1           \n",
    "            \n",
    "        # update epsilon using the following rule\n",
    "        epsilon = min_epsilon + (max_epsilon-min_epsilon) * np.exp(-decay*episode)           \n",
    "        ep_ene_min_data.append([ep_min_energy, episode])       \n",
    "        \n",
    "    # load interesting parameters to weight&Biases\n",
    "    wandb.log({\n",
    "        \"ep_ene_min_data\" : wandb.Table(data=ep_ene_min_data, columns=[\"energy minima\", \"episode\"]),\n",
    "        \"step_ene_min_data\" : wandb.Table(data=step_ene_min_data, columns=[\"energy\", \"step\"])\n",
    "    })        \n",
    "    return target_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, model, L): \n",
    "    env.reset()\n",
    "    energy = []\n",
    "    done = False\n",
    "\n",
    "    while not done: \n",
    "        observation = env.get_state()          \n",
    "        wandb.log({\"Test state\": wandb.Image(observation.reshape(L,L))})\n",
    "        \n",
    "        # Exploit best known action\n",
    "        predicted = model([observation.reshape(1,env.N,1), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n",
    "        while True:\n",
    "            #check to prevent flipping the same spin twice - only once!\n",
    "            action = np.argmax(predicted)\n",
    "            if env.get_state()[action,0] == 1:\n",
    "                    break;\n",
    "            predicted[action] = np.NINF\n",
    "\n",
    "        step_type, reward, discount, new_observation = env._step(action) \n",
    "        done = env._episode_ended\n",
    "        e = env.computeEnergy()\n",
    "        energy.append([new_observation, e])\n",
    "        \n",
    "    ground_state = np.min([e[1] for e in energy])         \n",
    "    return ground_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "5b441afef4b64307b5eb9e72ee3a64db",
    "deepnote_cell_height": 174,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "17c4b086",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:38:28.205022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-09 11:38:28.205066: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-09 11:38:28.205087: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9853aa3d459a): /proc/driver/nvidia/version does not exist\n",
      "2022-06-09 11:38:28.205505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 0  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 1  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 2  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 3  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 4  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 5  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 6  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 7  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 8  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 9  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 10  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 11  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 12  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 13  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 14  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 15  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 16  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 17  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 18  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 19  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 20  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 21  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 22  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 23  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 24  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 25  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 26  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 27  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 28  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 29  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 30  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 31  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 32  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 33  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 34  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 0  - step: 35  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 0  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 1  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 2  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 3  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 4  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 5  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 6  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 7  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 8  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 9  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 10  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 11  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 12  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 13  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 14  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 15  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 16  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 17  ++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/wandb/sdk/data_types/image.py:439: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data = (data - np.min(data)) / np.ptp(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 18  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 19  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 20  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 21  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 22  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 23  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 24  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 25  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 26  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 27  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 28  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 29  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 30  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 31  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 32  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 33  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 34  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 1  - step: 35  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 0  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 1  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 2  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 3  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 4  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 5  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 6  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 7  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 8  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 9  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 10  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 11  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 12  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 13  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 14  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 15  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 16  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 17  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 18  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 19  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 20  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 21  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 22  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 23  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 24  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 25  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 26  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 27  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 28  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 29  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 30  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 31  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 32  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 33  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 34  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 2  - step: 35  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 0  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 1  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 2  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 3  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 4  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 5  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 6  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 7  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 8  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 9  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 10  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 11  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 12  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 13  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 14  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 15  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 16  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 17  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 18  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 19  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 20  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 21  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 22  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 23  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 24  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 25  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 26  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 27  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 28  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 29  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 30  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 31  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 32  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 33  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 34  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 3  - step: 35  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 0  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 1  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 2  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 3  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 4  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 5  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 6  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 7  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 8  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 9  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 10  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 11  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 12  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 13  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 14  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 15  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 16  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 17  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 18  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 19  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 20  ++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 21  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 22  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 23  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 24  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 25  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 26  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 27  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 28  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 29  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 30  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['XENet_layer_0/dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['XENet_layer_0/dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "1/1 - 2s - loss: 5.3058 - _timestamp: 1654774715.0000 - _runtime: 11.0000 - 2s/epoch - 2s/step\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 31  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "1/1 - 0s - loss: 3.7928 - _timestamp: 1654774715.0000 - _runtime: 11.0000 - 151ms/epoch - 151ms/step\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 32  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "1/1 - 0s - loss: 5.6782 - _timestamp: 1654774715.0000 - _runtime: 11.0000 - 133ms/epoch - 133ms/step\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 33  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "1/1 - 0s - loss: 4.9209 - _timestamp: 1654774716.0000 - _runtime: 12.0000 - 151ms/epoch - 151ms/step\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 34  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "1/1 - 0s - loss: 5.7020 - _timestamp: 1654774716.0000 - _runtime: 12.0000 - 134ms/epoch - 134ms/step\n",
      "\n",
      "\n",
      "\t\t\t\t++++++++++++  episode: 4  - step: 35  ++++++++++++\n",
      "\n",
      "\t\t\t\t\t      +++++ Training +++++\n",
      "1/1 - 0s - loss: 4.5720 - _timestamp: 1654774716.0000 - _runtime: 12.0000 - 113ms/epoch - 113ms/step\n"
     ]
    }
   ],
   "source": [
    "L = 6\n",
    "D = 2\n",
    "N = L**D   \n",
    "environment = SG_env(L=L, D=D)\n",
    "\n",
    "# train and save the model\n",
    "trained_targetModel = main(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "1da016a58dc04fcdbca53063d4c15621",
    "deepnote_cell_height": 192,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "793c26c8",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# create a list of all possible states\\nensemble = [1, -1]\\nall_states = [x for x in product(ensemble, repeat=N)]\\n# compute the true energy minima\\ntrueEnergy = EnergyMinima(states=all_states, inter=environment.interaction, sparseAdj=environment.sp_AdjMat.indices.numpy())\\nprint(\"\\n\\nTrue energy minima\\n =>\", trueEnergy)\\n\\n# test the trained model\\nfoundEnergy = test(environment, trained_targetModel, L=L)\\nprint(\"\\nFound energy minima\\n =>\", foundEnergy, \"\\n\\n\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of all possible states\n",
    "ensemble = [1, -1]\n",
    "all_states = [x for x in product(ensemble, repeat=N)]\n",
    "# compute the true energy minima\n",
    "trueEnergy = EnergyMinima(states=all_states, inter=environment.interaction, sparseAdj=environment.sp_AdjMat.indices.numpy())\n",
    "print(\"\\n\\nTrue energy minima\\n =>\", trueEnergy)\n",
    "\n",
    "# test the trained model\n",
    "foundEnergy = test(environment, trained_targetModel, L=L)\n",
    "print(\"\\nFound energy minima\\n =>\", foundEnergy, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wandb.log({\\n    \"True Energy\"  : trueEnergy,\\n    \"Found Energy\" : foundEnergy\\n})\\nwandb.finish()'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.log({\n",
    "    \"True Energy\"  : trueEnergy,\n",
    "    \"Found Energy\" : foundEnergy\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "61c4312e-360b-4641-8d4d-1b368b7458c8",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
