{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "8abe6b57-b498-49fa-abc1-938c46fb211d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9c0d0462",
    "execution_start": 1654448721151,
    "execution_millis": 16126,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 549
   },
   "source": "import wandb\nimport numpy as np\nimport random\n\nfrom itertools import product\nfrom wandb.keras import WandbCallback\nfrom numba import jit\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dropout, Input, Flatten, Dense, Concatenate\nfrom tensorflow.keras.losses import CategoricalCrossentropy, Huber, MeanSquaredError\nfrom tensorflow.keras.metrics import categorical_accuracy\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.specs import array_spec, tensor_spec\n\nfrom spektral.layers import XENetDenseConv\nfrom spektral.utils.sparse import sp_matrix_to_sp_tensor\nfrom spektral.data import Graph\nfrom spektral.data.dataset import Dataset\nfrom spektral.data.loaders import BatchLoader",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5591f48e7014481f81da80c576075999",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "538d9111",
    "execution_start": 1654448978592,
    "execution_millis": 13761,
    "owner_user_id": "d8aa7146-c8e6-479e-9e49-136a058e4f85",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 262.375
   },
   "source": "# initialize wandb\n\nwandb.init(project=\"Test - D=2 & L=3\", \n           entity=\"locp\")\nprint(\"\\n\\n\\t\")",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9f38302a64cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m wandb.init(project=\"Test - D=2 & L=3\", \n\u001b[0;32m----> 4\u001b[0;31m            entity=\"locp\")\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interrupted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0merror_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0m_disable_warning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0m_silent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiet\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0m_entity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entity\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             )\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m# make sure login credentials get to the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             directive = (\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mno_offline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                     \u001b[0mno_create\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 )\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             )\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Predefined Functions ",
   "metadata": {
    "cell_id": "764f0144705b477b85c29b6f5e45277c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Adjacency Matrix ",
   "metadata": {
    "cell_id": "c54bacdda82b4818995e6a46948e40d2",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d422ff0c4fe74fe4a0a8ba07d42b05dd",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "63f6794e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 732
   },
   "source": "def Adj(D, L, sparse=False):\n    N = L**D\n\n    # create all nodes' coordinates\n    nodes = [x for x in np.ndindex(tuple(np.repeat(L,D)))]\n\n    # Pass from coordinate to node's index\n    # (h,...k,j,i) <=> index = h*L^(D-1) + ... + k*L^2 + j*L + i\n    mul = [L**i for i in reversed(range(D))]\n\n    # Creation of adjacency matrix \n    A_dense = []\n    # creation of a row for each node's coordinate \n    for node in nodes:       \n        temp_buffer = []\n        A_dense_row = [0]*N\n        # find the two nearest neighbours of the node along each dimension\n        for d in range(D):\n            temp=list(node)\n            temp[d]=((temp[d]+1)%L)\n            temp=np.inner(temp, mul)\n            temp_buffer.append(temp)    \n\n            temp=list(node)\n            temp[d]=((temp[d]-1)%L)\n            temp=np.inner(temp, mul)\n            temp_buffer.append(temp)\n      \n        temp_buffer=list(np.unique(np.array(temp_buffer), axis=0))   \n        for i in temp_buffer: A_dense_row[i]=1\n        A_dense.append(A_dense_row)\n    \n    # sparse=False => sparse adjacency matrix\n    # sparse=True => dense adjacency matrix\n    if sparse:\n        return sp_matrix_to_sp_tensor(np.array(A_dense))\n    else:\n        return np.array(A_dense)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Interaction Matrix ",
   "metadata": {
    "cell_id": "9633863cbc994b2eaf1281ad3f95844c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6f04bacceeba47cf9ea0943970aa23c6",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "3c2729e3",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 642
   },
   "source": "def J_inter(denseAdj):\n    N = denseAdj.shape[0]\n    sparseAdj = sp_matrix_to_sp_tensor(denseAdj)\n\n    # sparse adjacency matrix as a numpy array\n    edge=sparseAdj.indices.numpy()\n\n    # ordered numpy sparse adjacency matrix\n    un_edge=np.array([np.sort(i) for i in edge]) \n\n    # creation of the interaction array: (i,j) and (j,i) have the same Jij\n    inter=[]\n    for i in range(len(un_edge)):\n        equal=True\n        for j in range(i):\n            if np.array_equal(un_edge[i],un_edge[j]):\n                inter.append(inter[j])\n                equal=False\n                break\n        if equal: \n            inter.append(np.random.normal(0, 1))\n    \n    # creation of dense interaction matrix\n    inter_matrix = np.zeros((N,N))\n    counter = 0\n    for i, j in edge:\n        inter_matrix[i,j] = inter[counter]\n        counter += 1\n    return [np.array(inter).reshape(sparseAdj.indices.shape[0],1), inter_matrix.reshape((N,N,1))]\n    \n    # index of the returned list:\n    # 0 => interaction array\n    # 1 => interaction matrix (zero padded)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset",
   "metadata": {
    "cell_id": "150d8615cc1e4d9cbb9c6e2eec84b103",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1dd5d3a30c254ba489255b41c481d690",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5630b81b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 354
   },
   "source": "class MyDataset(Dataset):\n    def __init__(self, N_graph, X, Y, A, E, **kwargs):\n        self.X = X\n        self.Y = Y\n        self.N_graph = N_graph\n        self.A = A\n        self.E = E\n        super().__init__(**kwargs)\n\n    def read(self):\n        mydataset = []\n        for i in range(self.N_graph):\n            # list of Graph objects that will be used as input in the BatchLoader\n            mydataset.append(\n                    Graph(x=self.X[i], a=self.A[i], e=self.E[i], y=self.Y[i])      \n                    )\n        return mydataset",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Replay Memory Buffer",
   "metadata": {
    "cell_id": "7d5be6a8dc114757861855415712f148",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e181f4f5e9b845cc94a00c162e8e16fc",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8ae44cb4",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 390
   },
   "source": "# save n-step transitions (s_t; a_t; r_t,t+n; s_t+n) from the trajectory buffer\n\ndef get_replay_memory(trajectory_buffer, replay_memory):\n    n = len(trajectory_buffer)\n\n    states = np.array([transition[0] for transition in trajectory_buffer])\n    actions = np.array([transition[1] for transition in trajectory_buffer])\n    rewards = np.array([transition[2] for transition in trajectory_buffer])\n    done = np.array([transition[4] for transition in trajectory_buffer])\n    inter_matrix = trajectory_buffer[0][-1]\n    dense_AdjMat = trajectory_buffer[0][-2]\n\n    cum_reward = np.cumsum(rewards)\n\n    # creating the replay memory buffer from the trajectory one\n    # => (starting state, action performed, cumulative reward after n step from the starting one, state after n step from the starting one, episode ended, dense adjacency matrix of the episode, interaction matrix of the episode)\n    replay_memory.append([states[0], actions[0], cum_reward[n-1], states[n-1], done[n-1], dense_AdjMat, inter_matrix])\n\n    return replay_memory",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Real Energy Minima",
   "metadata": {
    "cell_id": "3b9dca89ae1e45a7a63ded986aa70996",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c0e39cae441b46248073b27d9d63221c",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "712751ce",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 480
   },
   "source": "@jit(nopython=True)\ndef computeEnergy(state, edge, interaction):  \n    energy = 0\n    for i in range(len(edge)):\n        energy -= interaction[i][0]*state[edge[i][0]][0]*state[edge[i][1]][0]\n    return energy/2\n\n\n@jit(nopython=True)\ndef EnergyMinima(L, D, inter):\n    N = L**D\n    sparseAdj = Adj(D, L, sparse=True).indices.numpy()\n    \n    # create a list of all possible states\n    ensemble = [1, -1]\n    states = [x for x in itertools.product(ensemble, repeat=N)]\n\n    # find the minimum energy for a fixed J_ij configuration\n    energy_min = np.inf\n    for state in states:\n        state = np.array(state).reshape(N,1)\n        state_energy = computeEnergy(state, sparseAdj, inter)\n        if state_energy<energy_min: energy_min=state_energy\n    return energy_min",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Environment ",
   "metadata": {
    "cell_id": "6e600cf72362489c987cfce6420ba23c",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "86b0cc302b154363947a634d2d672daa",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "bf0558b2",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1866
   },
   "source": "class SG_env(py_environment.PyEnvironment):\n\n  def __init__(self, L, D):\n    # Initialize environment attributes\n    # - action_spec: action declaration (integer from 0 to N-1)\n    # - observation_spec: state of the system declaration\n    # - episode_ended: flag for the end of the episode (all spin down)\n    # - state: state of the system (1 and -1 array)\n    self.N = L**D\n    self._action_spec = array_spec.BoundedArraySpec(\n        shape=(), dtype=np.int32, minimum=0, maximum=self.N-1, name='action')\n    self._observation_spec = array_spec.BoundedArraySpec(\n        shape=(self.N,1), dtype=np.int32, minimum=-1, maximum=1, name='observation')\n    self.sp_AdjMat = Adj(D, L, sparse=True)\n    self.dense_AdjMat = Adj(D, L, sparse=False)\n    self.interaction = J_inter(self.dense_AdjMat)[0]   \n    self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n    self._state = np.ones(shape=(self.N,1)).astype(\"int32\")\n    self._episode_ended = False\n\n  def get_state(self):\n    return self._state\n\n  def show_N(self):\n    return self.N  \n\n  def action_spec(self):\n    return self._action_spec\n\n  def observation_spec(self):\n    return self._observation_spec\n\n  def show_dense_AdjMat(self):\n    return self.dense_AdjMat\n\n  def show_sp_AdjMat(self):\n    return self.sp_AdjMat\n\n  def show_interaction(self):\n    return self.interaction\n\n  def show_inter_matrix(self):\n    return self.inter_matrix\n\n  # True => All spins = -1, False => otherwise\n  def __all_spins_down(self):\n    return np.all(self._state==-1)    \n\n  # Compute the reward of the chosen action \n  # reward = energy difference between consecutive states\n  # nns => nearest neighbours indexes\n  # nn_Js => nearest neighbours interactions' indexes\n  def computeReward(self, action):\n    nns = self.sp_AdjMat.indices[self.sp_AdjMat.indices[:,0]==action][:,1].numpy()\n    nn_Js = np.where(self.sp_AdjMat.indices[:,0]==action)[0]\n    nn_sum = 0\n    for i in range(len(nns)): nn_sum += self.interaction[nn_Js[i]]*self._state[nns[i],0]\n    reward = 2*nn_sum*self._state[action,0]\n    return reward[0]\n\n  # Compute the energy of the current state \n  def computeEnergy(self):\n    edge = self.sp_AdjMat.indices.numpy()\n    Nedge = len(edge)\n    energy = 0\n    for i in range(Nedge):\n        energy -= self.interaction[i][0]*self._state[edge[i][0]][0]*self._state[edge[i][1]][0]\n    return energy/2\n\n  # reset function: called when all spins are -1 => new episode\n  #                                              => all spins up (=1) and new interaction matrix (if needed)\n  def _reset(self):\n    self._state = np.ones(shape=(self.N,1)).astype(\"int32\")\n    #self.interaction = J_inter(self.dense_AdjMat)[0]    \n    #self.inter_matrix = J_inter(self.dense_AdjMat)[1]\n    self._episode_ended = False\n    return ts.restart(np.array(self._state, dtype=np.int32))\n\n  # step function: describe the process of applying the action selected by the agent\n  # ts.restart, ts.transition and ts.termination return a timestep \n  # containing step_type, reward, discount and observation\n  def _step(self, action):\n    if self._episode_ended:\n      return self.reset()\n\n    if self.__all_spins_down():\n      self._episode_ended = True\n    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==1):\n      self._state[action,0]=-1\n      rew = self.computeReward(action)\n      \n      if self.__all_spins_down():\n          self._episode_ended = True\n          return ts.termination(np.array(self._state, dtype=np.int32), reward=rew)\n      else:\n          return ts.transition(np.array(self._state, dtype=np.int32), reward=rew)\n    \n    elif (action>=0 and action<=self.N-1) and (self._state[action,0]==-1):\n      raise ValueError('Each spin can be flipped only once!')\n    else:\n      raise ValueError('`action` should be 0 up to N-1 - Spin Flip!')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Agent",
   "metadata": {
    "cell_id": "2b00da234ac54593b0738d6452581547",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6ef186888fa04be09638f4774e61a823",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "922c1210",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 876
   },
   "source": "# Agent (=> GNN+FNN) \n# N => number nodes\n# D => number dimensions\n# stack_channels => integer or list of integers, number of channels for the hidden layers\n# node_channels => integer, number of output channels for the nodes\n# edge_channels => integer, number of output channels for the edges\n# division_factor_dense => integer, gradually reduce the number of neurons for each dense layer\n# p_drop => float between 0 and 1, fraction of the input units to drop\n# N_xenet => Number of XENetDenseConv layers\n# N_dense => Number of Dense hidden layers\n\ndef agent(N,                                 \n          D,                                 \n          stack_channels=5,        \n          node_channels=3,\n          edge_channels=3,\n          division_factor_dense=4,\n          p_drop=0,\n          N_xenet=2,\n          N_dense=2,\n          activation=\"relu\",\n          regularizer=0):\n  inX = Input(shape=(N,1), name='Input Nodes')\n  inA = Input(shape=(N,N), name='Input Adj matrix')\n  inE = Input(shape=(N,N,1), name='Input Edges')\n  \n  X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n                               attention=True, node_activation=activation, edge_activation=activation, \n                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_0\")([inX, inA, inE])\n  for i in range(N_xenet-1):\n    X, E =  XENetDenseConv(stack_channels, node_channels, edge_channels,\n                               attention=True, node_activation=activation, edge_activation=activation, \n                               kernel_regularizer=l2(regularizer), name=\"XENet_layer_\"+str(i+1))([X, inA, E])\n  \n  # flat the updated X, E in order to feed the fully connected neural network (FNN)\n  flat_x, flat_e = Flatten(name=\"Nodes_encoding\")(X), Flatten(name=\"Edges_encoding\")(E)\n  out = Concatenate(axis=1, name=\"Concatenation\")([flat_x])    #,flat_e\n  \n  for i in range(N_dense):\n    out = Dense(out.shape.as_list()[1]//division_factor_dense, activation=activation, kernel_regularizer=l2(regularizer))(out)\n    out = Dropout(p_drop)(out)\n  out = Dense(N, activation=\"PReLU\", kernel_regularizer=l2(regularizer), name='Q-values')(out)\n  \n  model = Model([inX,inA,inE], out)\n  model.compile(optimizer=Adam(), loss=MeanSquaredError())\n  return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Training\n",
   "metadata": {
    "cell_id": "5713c3657aac41e4a3b3cd276685c9b2",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5624f1c5022a4d66b1c68d1a7075a781",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9b2c6fac",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 948
   },
   "source": "def train(env, replay_memory, model, target_model, done):\n    discount_factor = 0.618\n    \n    # skip the training if the number of samples in the replay \n    # memory is less than MIN_REPLAY_SIZE\n    MIN_REPLAY_SIZE = 100\n    if len(replay_memory) < MIN_REPLAY_SIZE:\n        return\n\n    # randomly select a number of samples from the replay memory equal to batch_size\n    batch_size = 60\n    mini_batch = random.sample(replay_memory, batch_size)\n\n    # sets of all interaction matrix and dense adjacency matrix in the batch\n    E = np.array([transition[-1] for transition in mini_batch])\n    A = np.array([transition[-2] for transition in mini_batch])\n\n    # current_states => set of all starting observations in the batch\n    # current_qs_list => predicted Q-values of the current_states by the model\n    current_states = np.array([transition[0] for transition in mini_batch])\n    current_qs_list = np.array(model.predict([current_states,A,E]))\n \n    # new_current_states => set of observations after performing n actions in the batch\n    # future_qs_list => predicted Q-values of the new_current_states by the target model\n    new_current_states = np.array([transition[3] for transition in mini_batch])\n    future_qs_list = np.array(target_model.predict([new_current_states,A,E]))\n    \n    # X => observations\n    # Y => 'label' of each observation: updated current_qs_list \n    X = []\n    Y = []\n    for index, (observation, action, reward, new_observation, done, dense_AdjMat, inter_matrix) in enumerate(mini_batch):\n        if not done:\n            new_q = reward + discount_factor*np.max(future_qs_list[index])\n        else:\n            new_q = reward\n\n        # update the Q-value corrisponding to the performed action\n        current_qs = current_qs_list[index]\n        current_qs[action] = new_q\n        \n        X.append(observation)\n        Y.append(current_qs)\n\n    # build the training dataset \n    train_data = MyDataset(N_graph=batch_size, X=X, Y=Y, A=A, E=E)\n    # use the BatchLoader to fit the model and WandbCallback to load the loss to Weight&Biases\n    loader = BatchLoader(train_data, node_level=False, epochs=50, batch_size=batch_size, shuffle=False) \n    model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, verbose=2,\n                callbacks = [WandbCallback()])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "2257d42bd8024c788fd22c3e04e7c7b1",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8a8b0363",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1902
   },
   "source": "def main(L=L, D=D):\n    L = 3\n    D = 2\n    env = SG_env(L=L, D=D)\n\n    # Epsilon-greedy algorithm in initialized at 1 \n    # => every step is random at the start\n    epsilon = 1          \n    max_epsilon = 1     \n    min_epsilon = 0.01  \n    decay = 0.01\n\n    # 1. Initialize the Target and Main models \n    # Main Model (updated every 3 steps)\n    model = agent(N=env.N, D=D, stack_channels=73, node_channels=20, edge_channels=20,\n                  N_xenet=1, N_dense=17, division_factor_dense=1)\n\n    # Target Model (updated at the end of every episode)\n    target_model = agent(N=env.N, D=D, stack_channels=73, node_channels=20, edge_channels=20,\n                         N_xenet=1, N_dense=17, division_factor_dense=1)\n    target_model.set_weights(model.get_weights())\n\n    energy_buffer = []\n    replay_memory = []\n    \n    step_buffer = 8\n    set_target = 9\n    train_episodes = 300\n    for episode in range(train_episodes):\n        # reset the variables at the beginning of an episode\n        trajectory_buffer = []                   \n        steps_to_update_target_model = 0\n        env.reset()\n        previous_obs = np.ones(shape=(env.N,1)).astype(\"int32\")\n        done = False  \n        check = np.arange(0,env.N)\n\n        while not done: \n            observation = env.get_state()          \n            print(\"\\n\\n\\t\\t\\t\\t++++++++++++  episode:\", episode,\" - step:\", steps_to_update_target_model, \" ++++++++++++\")\n\n            # 2. Explore using the Epsilon Greedy Exploration Strategy\n            random_number = np.random.rand()\n            if random_number <= epsilon:\n                # Explore\n                action = random.choice(check)\n\n            else:\n                # Exploit best known action\n                predicted = model([observation.reshape(1,env.N,1), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n                while True:\n                    # check to prevent flipping the same spin twice - only once!\n                    action = np.argmax(predicted)\n                    if env.get_state()[action,0] == 1:\n                            break;\n                    predicted[action] = np.NINF\n\n            # remove the choosen action from check array\n            check = np.setdiff1d(check, action)\n\n            # perform the action on the environment and get the updated parameters\n            step_type, reward, discount, new_observation = env._step(action)\n            done = env._episode_ended\n            e = env.computeEnergy()\n\n            # save the parameter in the buffer\n            trajectory_buffer.append([previous_obs, action, reward, new_observation, done, e, env.dense_AdjMat, env.inter_matrix])\n            energy_buffer.append([episode, new_observation, env.interaction, e])  \n            \n            # load interesting parameters to weight&Biases\n            wandb.log({\n                \"Episode\": energy_buffer[episode*env.N+steps_to_update_target_model][0],\n                \"Step\": episode*env.N+steps_to_update_target_model,\n                \"New observation\": wandb.Image(energy_buffer[episode*env.N+steps_to_update_target_model][1].reshape(L,L)),\n                \"J interactions\": energy_buffer[episode*env.N+steps_to_update_target_model][2],\n                \"Energy\": energy_buffer[episode*env.N+steps_to_update_target_model][3]\n            })\n\n            # fill the replay memory buffer\n            if steps_to_update_target_model >= step_buffer:   \n                replay_memory = get_replay_memory(trajectory_buffer, replay_memory)\n                trajectory_buffer = trajectory_buffer[1:]\n\n            # 3. Update the Main Network using the Bellman Equation  \n            #if (steps_to_update_target_model%L==0 and steps_to_update_target_model!=0) or done:\n            print(\"\\n\\t\\t\\t\\t\\t      +++++ Training +++++\")\n            train(env, replay_memory, model, target_model, done)\n  \n            previous_obs = new_observation\n\n            # Copying main network weights to the target network\n            # weights at the end of the episode\n            if done:\n                if episode >= set_target:\n                    target_model.set_weights(model.get_weights())\n                break\n\n            steps_to_update_target_model += 1\n\n        # update epsilon using the following rule\n        epsilon = min_epsilon+ (max_epsilon -min_epsilon) * np.exp(-decay *episode)\n\n    return [target_model, env]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "5b441afef4b64307b5eb9e72ee3a64db",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "17c4b086",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 174
   },
   "source": "L = 3\nD = 2\nresult = main(L=L, D=D)\n\n# save the result of the training\ntrainedModel = result[0]\nenvironment = result[1]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Testing",
   "metadata": {
    "cell_id": "8da78072b1ea4950b34bcd9da08d7ad2",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6af546f57e7241bd9b58fd80d9354dee",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "fb86de1e",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 516
   },
   "source": "def test(env, model, L): \n    env.reset()\n    energy = []\n    done = False\n\n    while not done: \n        observation = env.get_state()          \n        wandb.log({\n            \"Test state\": wandb.Image(observation.reshape(L,L)),\n        })\n        # Exploit best known action\n        predicted = model([observation.reshape(1,env.N,1), env.dense_AdjMat.reshape(1,env.N,env.N), env.inter_matrix.reshape(1,env.N,env.N,1)], training=False).numpy()[0]\n        while True:\n            #check to prevent flipping the same spin twice - only once!\n            action = np.argmax(predicted)\n            if env.get_state()[action,0] == 1:\n                    break;\n            predicted[action] = np.NINF\n\n        step_type, reward, discount, new_observation = env._step(action) \n        done = env._episode_ended\n        e = env.computeEnergy()\n        energy.append([new_observation, e])\n        ground_state = energy[energy[1]==min(energy[1])]\n\n    return ground_state",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1da016a58dc04fcdbca53063d4c15621",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "793c26c8",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 192
   },
   "source": "# compute the true energy minima\ntrueEnergy = EnergyMinima(L=L, D=D, inter=environment.interaction)\nprint(\"True energy minima\\n =>\", trueEnergy)\n\n# test the trained model\nfoundEnergy = test(environment, trainedModel, L=L)\nprint(\"Found energy minima\\n =>\", foundEnergy)\nwandb.finish()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d3e24c81-aab9-4b2e-80ab-106a9c168933' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {},
  "deepnote_notebook_id": "61c4312e-360b-4641-8d4d-1b368b7458c8",
  "deepnote_execution_queue": []
 }
}